\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Oct 26, 2025 & 1.0 & Added Section 3 \\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

The V\&V plan described in this document applies to the VoiceBridge application. VoiceBridge is an accessibility tool that is built around speech-to-text, designed for individuals with impaired or atypical speech caused by neurological or motor speech disorders. Using a customizable model, it converts slurred or non-standard speech into accurate text and control commands on standard consumer devices. The system integrates seamlessly with commonly used technology, including web browsers, enabling open-ended, customizable control for tasks such as online shopping, browsing, and digital communication. By allowing users to operate everyday applications with their own speech patterns, the product promotes greater autonomy, accessibility, and independence.

\subsection{Objectives}

The primary objective of VoiceBridge is to accurately capture and interpret the intended meaning of speech from users with impaired or atypical articulation, ensuring that their spoken input is reliably translated into the correct text or device actions. A critical focus is the safe and precise mapping of user intent, particularly when controlling web browsers and other general-purpose software environments where unintended or harmful actions could have significant consequences. Because the system is dependent on non-deterministic AI models, robust safeguards, validation layers, and command verification mechanisms will be implemented to prevent misinterpretation, accidental activation, or malicious exploitation. The project also strives to maintain usability and accessibility by operating on standard consumer hardware and providing clear, transparent feedback to the user. Ultimately, the goal is to empower users with meaningful autonomy while maintaining strict control, reliability, and safety in all system interactions.

\subsection{Challenge Level and Extras}

Challenges not applicable to V\&V plan.

\subsection{Relevant Documentation}

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.  You can create BibTeX entries for your
%   documents and within those entries include a hyperlink to the documents.}

% \citet{SRS}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

Below is a list of documents that can be referenced for further information, as well as short descriptions highlighting their relevance to V\&V
\begin{itemize}
  \item \href{https://github.com/speech-buddies/VoiceBridge/blob/main/docs/ProblemStatementAndGoals/ProblemStatement.pdf}{Problem Statement \& Goals}: This document summarizes the core objectives of VoiceBridge and the problem it aims to address. These initial goals lay the foundation for the subsequent requirements that the V\&V aims to ensure.
  \item \href{https://github.com/speech-buddies/VoiceBridge/blob/main/docs/SRS-Volere/SRS.pdf}{Software Requirements Specification}: This document goes into depth about the project specifications (user identification, business cases) as well as the functional and non-functional requirements that the V\&V validates through the outlined testing plan. 
  \item Design Document (To be written): This document will summarize the core modules of VoiceBridge, highlighting the components that needed to be directly and indirectly tested, as defined in the V\&V.
\end{itemize}



\section{Plan}

This section describes how the VoiceBridge team will verify and validate the system throughout its lifecycle. It outlines responsibilities, reviews, and testing activities that ensure each stage, from the SRS to it’s implementation. The plan emphasizes practical verification aligned with our project scope.

\subsection{Verification and Validation Team}

\begin{itemize}
    \item \textbf{Kelvin Yu:} Lead Tester. Coordinates overall V\&V activities and integration testing across the ASR, intent mapping, and browser components.
    \item \textbf{Mazen Youssef:} Functional Tester. Verifies core functional requirements (FR1--FR5) including audio capture, transcription, and command execution.
    \item \textbf{Rawan Mahdi:} UI and Usability Tester. Tests the front-end interface for accessibility, feedback clarity, and overall user experience.
    \item \textbf{Luna Aljammal:} Non-Functional Tester. Assesses performance, latency, and privacy compliance; maintains testing documentation and traceability.
    \item \textbf{Dr.~Christian Brodbeck (Supervisor):} Oversees the V\&V process, providing feedback on testing and alignment with project goals.
\end{itemize}


\subsection{SRS Verification}

\subsubsection*{3.2.1 Requirements Validation}
Each functional and non-functional requirement will be verified for clarity, feasibility, and measurability. A formal SRS Review Checklist will be applied to Sections~10--16, focusing on fit criteria, ambiguity, and traceability. Automated verification will be performed for measurable criteria (e.g., transcription latency, confidence scores) using \texttt{PyTest}, while manual validation will confirm qualitative attributes such as user feedback clarity.

\subsubsection*{3.2.2 Supervisor Review}
A structured review session will be held with Dr.~Christian Brodbeck. The meeting will consist of:

\begin{enumerate}
    \item A concise summary of all functional and safety-related requirements (IR, PRR, ACR, IMR).
    \item System and use case diagrams for visual reference.
    \item Specific discussion prompts on potentially ambiguous or high-risk requirements.
\end{enumerate}

During the meeting, we will assess correctness, feasibility, and alignment with user needs. All comments will be logged as GitHub issues and tracked under the \textit{SRS Verification} label for resolution.

\subsubsection*{3.2.3 Prototype-Based Validation}
A low-fidelity prototype of VoiceBridge’s transcription interface and feedback module will be used to validate usability-related requirements (e.g., accessibility of controls, clarity of feedback, and response timing). Test participants will perform scripted scenarios derived from core functional requirements, such as initiating live transcription, adjusting speech sensitivity, and reviewing transcript accuracy.Results will be compared against defined success metrics (e.g., \textit{task completion within 5 seconds} or \textit{90\% accuracy threshold}).

\subsubsection*{3.2.4 Continuous Verification}
To ensure ongoing alignment between the SRS and the evolving design, biweekly verification reviews will be conducted. These sessions will:
\begin{itemize}
    \item Assess the impact of requirement modifications.
    \item Re-verify modified requirements using the checklist to confirm consistency and completeness.
\end{itemize}


\begin{table}[H]
\centering
\caption{SRS Verification Checklist}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{5cm}|p{9cm}|}
\hline
\textbf{Criteria} & \textbf{Verification Activities} \\ \hline

Requirements Validation &
\(\square\) Apply checklist to all SRS sections \newline
\(\square\) Execute automated + manual verifications \\ \hline

Supervisor Review &
\(\square\) Prepare and distribute review materials \newline
\(\square\) Conduct formal walkthrough \newline
\(\square\) Record findings as GitHub issues \\ \hline

Prototype-Based Validation &
\(\square\) Develop interactive prototype \newline
\(\square\) Run scenario-based usability tests \newline
\(\square\) Compare results against success metrics \\ \hline

Continuous Verification &
\(\square\) Hold biweekly review meetings \newline
\(\square\) Update documentation and traceability records \newline
\(\square\) Re-inspect modified requirements \\ \hline

\end{tabular}
\end{table}


\subsection{Design Verification}

Design verification confirms that the VoiceBridge architecture and modules meet all verified requirements from the SRS. Planned Verification Activities:

\begin{itemize}
    \item Design Review Meeting: \\
    Conduct a structured walkthrough of the Module Interface Specification with Dr.~Brodbeck. Each module will be checked against SRS FR1--FR5 and non-functional requirements. Findings will be documented and tracked in GitHub under \textit{Design Verification}.
    
    \item Checklist Inspection: \\
    Verify that data flows align with the SRS Data Dictionary and that control logic matches the Business Use Cases. Confirm consistency in naming, data handling, and error management.
    
    \item Interface Validation: \\
    Use UI mock-ups to confirm compliance with Accessibility and Safety-Critical requirements. Validate color contrast, keyboard navigation, and error feedback clarity.
\end{itemize}

\begin{table}[H]
\centering
\caption{Design Verification Checklist}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{5cm}|p{9cm}|}
\hline
Criteria & Verification Activities \\ \hline
Design Review &
\(\square\) Cross-check each module with SRS requirements \newline
\(\square\) Log findings in GitHub \\ \hline
Checklist Inspection &
\(\square\) Validate data flows and logic consistency \newline
\(\square\) Check naming and error handling \\ \hline
Interface Validation &
\(\square\) Review mock-ups for accessibility and safety compliance \\ \hline
\end{tabular}
\end{table}

\subsection{Verification and Validation Plan Verification}

The Verification and Validation (V\&V) Plan will undergo its own verification process to ensure that it is accurate, complete, and consistent with project standards. Planned Verification Activities:

\begin{itemize}
    \item Peer Inspection: \\
    All team members will review the plan using a standardized checklist to evaluate completeness of scope, requirement traceability, and feasibility of proposed activities.

    \item Supervisor Approval: \\
    The finalized document will be submitted to Dr.~Brodbeck for formal review. Feedback will confirm that verification procedures align with course and project expectations.

    \item Issue Logging and Revision Tracking: \\
    Any missing information, inconsistencies, or suggested improvements will be logged as GitHub issues. Updates will be reviewed and approved before submission.
\end{itemize}

\begin{table}[H]
\centering
\caption{V\&V Plan Verification Checklist}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{5.5cm}|p{8cm}|}
\hline
Criteria & Verification Activities \\ \hline
Peer Inspection &
\(\square\) Verify traceability and feasibility  \newline
\(\square\) Review plan scope, objectives, and completeness \\ \hline
Supervisor Approval &
\(\square\) Submit plan for instructor feedback \newline
\(\square\) Incorporate required revisions \\ \hline
Issue Logging and Tracking &
\(\square\) Record suggested updates in GitHub \newline
\(\square\) Confirm all revisions are reviewed and approved \\ \hline
\end{tabular}
\end{table}

\subsection{Implementation Verification}

Implementation verification ensures that VoiceBridge’s source code correctly implements the approved design and satisfies all verified requirements. Planned Verification Activities:

\begin{itemize}
    \item Unit and Integration Testing: \\
    Automated tests will verify core functional requirements (SRS~§10, FR1--FR5), covering audio input, speech-to-text, intent mapping, and command execution. End-to-end PUC flows (SRS~§8.2) will be validated through CI pipelines with coverage tracking.

    \item Static Analysis and Peer Review: \\
    Code reviews will check compliance with maintainability and security requirements (SRS~§15--16) and confirm mitigation of hazards listed in the Hazard Analysis (e.g., IR1, IMR1--IMR3). Issues will be tracked on GitHub for traceability.

    \item Automated Code Quality Tools: \\
    PEP~8 and ESLint will enforce code quality and consistency, supporting maintainability goals (SRS~§15.1).
\end{itemize}

\begin{table}[H]
\centering
\caption{Implementation Verification Checklist}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
Criteria & Verification Activities \\ \hline
Unit \& Integration Testing &
\(\square\) Track coverage and test results \newline
\(\square\) Develop automated test suites for all modules \\ \hline
Static Analysis \& Peer Review &
\(\square\) Conduct code inspections \newline
\(\square\) Log and resolve defects in GitHub \\ \hline
Code Quality Tools &
\(\square\) Enforce style rules with PEP~8 and ESLint \newline
\(\square\) Verify consistent formatting and documentation \\ \hline
\end{tabular}
\end{table}


\subsection{Automated Testing and Verification Tools}

To support consistent and repeatable verification, the following tools will be used.

\begin{table}[H]
\centering
\caption{Automated Testing and Verification Tools}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{4cm}|p{10cm}|}
\hline
Tool & Purpose \\ \hline
PyTest / unittest & Automated unit tests for ASR and intent mapping modules (Python). \\ \hline
GitHub Actions & Continuous integration pipeline executing test suites on each commit. \\ \hline
ESLint / Prettier & Enforces style and code quality standards for JavaScript/TypeScript. \\ \hline
pytest-cov / Codecov & Collects and reports test coverage metrics; uploads reports per build. \\ \hline
WAVE / axe Accessibility Scanner & Evaluates UI compliance with WCAG~2.1~AA. \\ \hline
\end{tabular}
\end{table}

Coverage and Linting Summary

\begin{itemize}
    \item \texttt{pytest-cov} will generate line and branch coverage reports.
    \item Coverage summaries will appear on pull requests and weekly trend reports in the repository.
    \item Linters (\texttt{ruff}, \texttt{eslint}, and \texttt{prettier}) will ensure consistent style and catch syntax or accessibility issues before merging.
\end{itemize}

\subsection{Software Validation}

Validation confirms that VoiceBridge meets user needs and operates as intended in real-world conditions. Approaches:

\begin{itemize}
    \item Stakeholder Feedback and Demo Validation: \\
    Rev~0 and Rev~1 demos will be used to validate against user personas and supervisor expectations.

    \item User Testing: \\
    Collect feedback from two to three target users (simulated or actual) performing core use cases (PUC-1 to PUC-5) with success metrics on accuracy and ease of use.

    \item Functional Testing: \\
    Validate VoiceBridge’s performance and behavior against comparable open-source Python speech-processing projects to confirm functional correctness.
\end{itemize}


\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs.  Output is not how you
are going to return the results of the test.  The output is the expected
result.}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}