\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Oct 26, 2025 & 1.0 & Added Section 3 \\
Oct 27, 2025 & 1.0 & Added Sections 1,2, and 4.1\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

The V\&V plan described in this document applies to the VoiceBridge application. VoiceBridge is an accessibility tool that is built around speech-to-text, designed for individuals with impaired or atypical speech caused by neurological or motor speech disorders. Using a customizable model, it converts slurred or non-standard speech into accurate text and control commands on standard consumer devices. The system integrates seamlessly with commonly used technology, including web browsers, enabling open-ended, customizable control for tasks such as online shopping, browsing, and digital communication. By allowing users to operate everyday applications with their own speech patterns, the product promotes greater autonomy, accessibility, and independence.

\subsection{Objectives}

The primary objective of VoiceBridge is to accurately capture and interpret the intended meaning of speech from users with impaired or atypical articulation, ensuring that their spoken input is reliably translated into the correct text or device actions. A critical focus is the safe and precise mapping of user intent, particularly when controlling web browsers and other general-purpose software environments where unintended or harmful actions could have significant consequences. Because the system is dependent on non-deterministic AI models, robust safeguards, validation layers, and command verification mechanisms will be implemented to prevent misinterpretation, accidental activation, or malicious exploitation. The project also strives to maintain usability and accessibility by operating on standard consumer hardware and providing clear, transparent feedback to the user. Ultimately, the goal is to empower users with meaningful autonomy while maintaining strict control, reliability, and safety in all system interactions.

\subsection{Challenge Level and Extras}

Challenges not applicable to V\&V plan.

\subsection{Relevant Documentation}

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.  You can create BibTeX entries for your
%   documents and within those entries include a hyperlink to the documents.}

% \citet{SRS}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

Below is a list of documents that can be referenced for further information, as well as short descriptions highlighting their relevance to V\&V
\begin{itemize}
  \item \href{https://github.com/speech-buddies/VoiceBridge/blob/main/docs/ProblemStatementAndGoals/ProblemStatement.pdf}{Problem Statement \& Goals}: This document summarizes the core objectives of VoiceBridge and the problem it aims to address. These initial goals lay the foundation for the subsequent requirements that the V\&V aims to ensure.
  \item \href{https://github.com/speech-buddies/VoiceBridge/blob/main/docs/SRS-Volere/SRS.pdf}{Software Requirements Specification}: This document goes into depth about the project specifications (user identification, business cases) as well as the functional and non-functional requirements that the V\&V validates through the outlined testing plan. 
  \item Design Document (To be written): This document will summarize the core modules of VoiceBridge, highlighting the components that needed to be directly and indirectly tested, as defined in the V\&V.
\end{itemize}



\section{Plan}

This section describes how the VoiceBridge team will verify and validate the system throughout its lifecycle. It outlines responsibilities, reviews, and testing activities that ensure each stage, from the SRS to it’s implementation. The plan emphasizes practical verification aligned with our project scope.

\subsection{Verification and Validation Team}

\begin{itemize}
    \item \textbf{Kelvin Yu:} Lead Tester. Coordinates overall V\&V activities and integration testing across the ASR, intent mapping, and browser components.
    \item \textbf{Mazen Youssef:} Functional Tester. Verifies core functional requirements (FR1--FR5) including audio capture, transcription, and command execution.
    \item \textbf{Rawan Mahdi:} UI and Usability Tester. Tests the front-end interface for accessibility, feedback clarity, and overall user experience.
    \item \textbf{Luna Aljammal:} Non-Functional Tester. Assesses performance, latency, and privacy compliance; maintains testing documentation and traceability.
    \item \textbf{Dr.~Christian Brodbeck (Supervisor):} Oversees the V\&V process, providing feedback on testing and alignment with project goals.
\end{itemize}


\subsection{SRS Verification}

\subsubsection*{3.2.1 Requirements Validation}
Each functional and non-functional requirement will be verified for clarity, feasibility, and measurability. A formal SRS Review Checklist will be applied to Sections~10--16, focusing on fit criteria, ambiguity, and traceability. Automated verification will be performed for measurable criteria (e.g., transcription latency, confidence scores) using \texttt{PyTest}, while manual validation will confirm qualitative attributes such as user feedback clarity.

\subsubsection*{3.2.2 Supervisor Review}
A structured review session will be held with Dr.~Christian Brodbeck. The meeting will consist of:

\begin{enumerate}
    \item A concise summary of all functional and safety-related requirements (IR, PRR, ACR, IMR).
    \item System and use case diagrams for visual reference.
    \item Specific discussion prompts on potentially ambiguous or high-risk requirements.
\end{enumerate}

During the meeting, we will assess correctness, feasibility, and alignment with user needs. All comments will be logged as GitHub issues and tracked under the \textit{SRS Verification} label for resolution.

\subsubsection*{3.2.3 Prototype-Based Validation}
A low-fidelity prototype of VoiceBridge’s transcription interface and feedback module will be used to validate usability-related requirements (e.g., accessibility of controls, clarity of feedback, and response timing). Test participants will perform scripted scenarios derived from core functional requirements, such as initiating live transcription, adjusting speech sensitivity, and reviewing transcript accuracy.Results will be compared against defined success metrics (e.g., \textit{task completion within 5 seconds} or \textit{90\% accuracy threshold}).

\subsubsection*{3.2.4 Continuous Verification}
To ensure ongoing alignment between the SRS and the evolving design, biweekly verification reviews will be conducted. These sessions will:
\begin{itemize}
    \item Assess the impact of requirement modifications.
    \item Re-verify modified requirements using the checklist to confirm consistency and completeness.
\end{itemize}


\begin{table}[H]
\centering
\caption{SRS Verification Checklist}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{5cm}|p{9cm}|}
\hline
\textbf{Criteria} & \textbf{Verification Activities} \\ \hline

Requirements Validation &
\(\square\) Apply checklist to all SRS sections \newline
\(\square\) Execute automated + manual verifications \\ \hline

Supervisor Review &
\(\square\) Prepare and distribute review materials \newline
\(\square\) Conduct formal walkthrough \newline
\(\square\) Record findings as GitHub issues \\ \hline

Prototype-Based Validation &
\(\square\) Develop interactive prototype \newline
\(\square\) Run scenario-based usability tests \newline
\(\square\) Compare results against success metrics \\ \hline

Continuous Verification &
\(\square\) Hold biweekly review meetings \newline
\(\square\) Update documentation and traceability records \newline
\(\square\) Re-inspect modified requirements \\ \hline

\end{tabular}
\end{table}


\subsection{Design Verification}

Design verification confirms that the VoiceBridge architecture and modules meet all verified requirements from the SRS. Planned Verification Activities:

\begin{itemize}
    \item Design Review Meeting: \\
    Conduct a structured walkthrough of the Module Interface Specification with Dr.~Brodbeck. Each module will be checked against SRS FR1--FR5 and non-functional requirements. Findings will be documented and tracked in GitHub under \textit{Design Verification}.
    
    \item Checklist Inspection: \\
    Verify that data flows align with the SRS Data Dictionary and that control logic matches the Business Use Cases. Confirm consistency in naming, data handling, and error management.
    
    \item Interface Validation: \\
    Use UI mock-ups to confirm compliance with Accessibility and Safety-Critical requirements. Validate color contrast, keyboard navigation, and error feedback clarity.
\end{itemize}

\begin{table}[H]
\centering
\caption{Design Verification Checklist}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{5cm}|p{9cm}|}
\hline
Criteria & Verification Activities \\ \hline
Design Review &
\(\square\) Cross-check each module with SRS requirements \newline
\(\square\) Log findings in GitHub \\ \hline
Checklist Inspection &
\(\square\) Validate data flows and logic consistency \newline
\(\square\) Check naming and error handling \\ \hline
Interface Validation &
\(\square\) Review mock-ups for accessibility and safety compliance \\ \hline
\end{tabular}
\end{table}

\subsection{Verification and Validation Plan Verification}

The Verification and Validation (V\&V) Plan will undergo its own verification process to ensure that it is accurate, complete, and consistent with project standards. Planned Verification Activities:

\begin{itemize}
    \item Peer Inspection: \\
    All team members will review the plan using a standardized checklist to evaluate completeness of scope, requirement traceability, and feasibility of proposed activities.

    \item Supervisor Approval: \\
    The finalized document will be submitted to Dr.~Brodbeck for formal review. Feedback will confirm that verification procedures align with course and project expectations.

    \item Issue Logging and Revision Tracking: \\
    Any missing information, inconsistencies, or suggested improvements will be logged as GitHub issues. Updates will be reviewed and approved before submission.
\end{itemize}

\begin{table}[H]
\centering
\caption{V\&V Plan Verification Checklist}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{5.5cm}|p{8cm}|}
\hline
Criteria & Verification Activities \\ \hline
Peer Inspection &
\(\square\) Verify traceability and feasibility  \newline
\(\square\) Review plan scope, objectives, and completeness \\ \hline
Supervisor Approval &
\(\square\) Submit plan for instructor feedback \newline
\(\square\) Incorporate required revisions \\ \hline
Issue Logging and Tracking &
\(\square\) Record suggested updates in GitHub \newline
\(\square\) Confirm all revisions are reviewed and approved \\ \hline
\end{tabular}
\end{table}

\subsection{Implementation Verification}

Implementation verification ensures that VoiceBridge’s source code correctly implements the approved design and satisfies all verified requirements. Planned Verification Activities:

\begin{itemize}
    \item Unit and Integration Testing: \\
    Automated tests will verify core functional requirements (SRS~§10, FR1--FR5), covering audio input, speech-to-text, intent mapping, and command execution. End-to-end PUC flows (SRS~§8.2) will be validated through CI pipelines with coverage tracking.

    \item Static Analysis and Peer Review: \\
    Code reviews will check compliance with maintainability and security requirements (SRS~§15--16) and confirm mitigation of hazards listed in the Hazard Analysis (e.g., IR1, IMR1--IMR3). Issues will be tracked on GitHub for traceability.

    \item Automated Code Quality Tools: \\
    PEP~8 and ESLint will enforce code quality and consistency, supporting maintainability goals (SRS~§15.1).
\end{itemize}

\begin{table}[H]
\centering
\caption{Implementation Verification Checklist}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
Criteria & Verification Activities \\ \hline
Unit \& Integration Testing &
\(\square\) Track coverage and test results \newline
\(\square\) Develop automated test suites for all modules \\ \hline
Static Analysis \& Peer Review &
\(\square\) Conduct code inspections \newline
\(\square\) Log and resolve defects in GitHub \\ \hline
Code Quality Tools &
\(\square\) Enforce style rules with PEP~8 and ESLint \newline
\(\square\) Verify consistent formatting and documentation \\ \hline
\end{tabular}
\end{table}


\subsection{Automated Testing and Verification Tools}

To support consistent and repeatable verification, the following tools will be used.

\begin{table}[H]
\centering
\caption{Automated Testing and Verification Tools}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{4cm}|p{10cm}|}
\hline
Tool & Purpose \\ \hline
PyTest / unittest & Automated unit tests for ASR and intent mapping modules (Python). \\ \hline
GitHub Actions & Continuous integration pipeline executing test suites on each commit. \\ \hline
ESLint / Prettier & Enforces style and code quality standards for JavaScript/TypeScript. \\ \hline
pytest-cov / Codecov & Collects and reports test coverage metrics; uploads reports per build. \\ \hline
WAVE / axe Accessibility Scanner & Evaluates UI compliance with WCAG~2.1~AA. \\ \hline
\end{tabular}
\end{table}

Coverage and Linting Summary

\begin{itemize}
    \item \texttt{pytest-cov} will generate line and branch coverage reports.
    \item Coverage summaries will appear on pull requests and weekly trend reports in the repository.
    \item Linters (\texttt{ruff}, \texttt{eslint}, and \texttt{prettier}) will ensure consistent style and catch syntax or accessibility issues before merging.
\end{itemize}

\subsection{Software Validation}

Validation confirms that VoiceBridge meets user needs and operates as intended in real-world conditions. Approaches:

\begin{itemize}
    \item Stakeholder Feedback and Demo Validation: \\
    Rev~0 and Rev~1 demos will be used to validate against user personas and supervisor expectations.

    \item User Testing: \\
    Collect feedback from two to three target users (simulated or actual) performing core use cases (PUC-1 to PUC-5) with success metrics on accuracy and ease of use.

    \item Functional Testing: \\
    Validate VoiceBridge’s performance and behavior against comparable open-source Python speech-processing projects to confirm functional correctness.
\end{itemize}


\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

\subsection{Tests for Functional Requirements}

The subsections below outline tests corresponding to the SRS Functional Requirements (FR1--FR5).
Each test has a unique ID and clear execution steps.

\subsubsection{4.1.1 Tests for FR1 — Accept Speech Audio via Microphone}

\textbf{Goal:} Verify live microphone capture across platforms at $\geq$16 kHz.

\begin{enumerate}

\item{test-FR1-MIC-1 Valid Microphone Capture (Desktop)\\}

\textbf{Type:} Manual

\textbf{Initial State:} App installed; permissions not yet granted.

\textbf{Input/Condition:} Speak a short phrase (\textit{``Testing one two three''}) into the default OS microphone on a Windows OS laptop.

\textbf{Output/Result:} App requests microphone permission; once granted, waveform/level indicator appears and raw audio frames are buffered at $\geq$16 kHz.

\textbf{How test will be performed:} Launch app $\to$ start capture $\to$ verify UI level meter moves; export a short recording and inspect metadata (sample rate $\geq$16 kHz).

\item{test-FR1-MIC-2 Mobile Permission \& Capture\\}

\textbf{Type:} Manual

\textbf{Initial State:} App freshly installed on iOS and Android.

\textbf{Input/Condition:} First-time launch and attempt to record.

\textbf{Output/Result:} OS permission prompt shown; after approval, capture begins; denial shows friendly error and retry path.

\textbf{How test will be performed:} Deny once (expect error + guidance), then allow (expect capture + levels).

\item{test-FR1-MIC-3 Default Device Selection \& Fallback\\}

\textbf{Type:} Manual

\textbf{Initial State:} Multiple audio devices present (laptop microphone + USB microphone).

\textbf{Input/Condition:} Unplug/plug devices while capturing.

\textbf{Output/Result:} The system selects the current default audio input device. If the device disconnects, it automatically falls back to another available microphone and notifies the user.

\textbf{How test will be performed:} Start capture $\to$ unplug active microphone $\to$ observe fallback notice $\to$ confirm continued capture.

\end{enumerate}

\subsubsection{Tests for FR2 — Convert Impaired Speech to Text $\geq$80\% Accuracy (MVP)}
\textbf{Goal:} Validate transcription accuracy on impaired speech and common commands.


\begin{enumerate}

\item{test-FR2-ASR-1 Dataset Evaluation vs Baseline\\}

\textbf{Type:} Automated \(batch evaluation\)

\textbf{Initial State:} Curated impaired-speech test set with references; chosen baseline model.

\textbf{Input/Condition:} Run system and baseline over full test set.

\textbf{Output/Result:} System achieves $\geq$80\% WER reduction vs baseline \textbf{or} meets defined WER delta per SRS; report includes per-speaker metrics.

\textbf{How test will be performed:} Evaluation script computes WER, CER; export CSV; assert threshold met.

\item{test-FR2-ASR-2: Common Command Accuracy $\geq$80\%\\}

\textbf{Type:} Automated \(scripted playback\)

\textbf{Initial State:} Command list of 50+ common intents; audio variants (different speakers/noise).

\textbf{Input/Condition:} Play each command sample to the system.

\textbf{Output/Result:} Meets $\geq$80\% correct textual outputs (exact or accepted synonyms).

\textbf{How test will be performed:} Align decoded text with ground truth (string/intent match); assert $\geq$80\% pass rate.

\end{enumerate}

\subsubsection{4.1.3 Tests for FR3 — Display Transcription for Verification}


\textbf{Goal:} Real-time text display within 2 seconds for $\geq$95\% of trials.


\begin{enumerate}

\item{test-FR3-UI-1: Real-Time Display Latency\\}

\textbf{Type:} Automated (instrumented)

\textbf{Initial State:} Logging timestamps at (a) audio frame arrival, (b) first token shown.

\textbf{Input/Condition:} 100 short utterances across platforms.

\textbf{Output/Result:} Time-to-first-text $\leq$2 s in $\geq$95\% of trials.

\textbf{How test will be performed:} Run scripted playback or live reads with a timer, store successful and unsuccessful trials and calculate percentage.

\item{test-FR3-UI-2: Continuous Update \& Finalization\\}

\textbf{Type:} Manual

\textbf{Initial State:} Streaming UI enabled.

\textbf{Input/Condition:} Speak a 10--15 s sentence; include pauses and restarts.

\textbf{Output/Result:} UI shows incremental text as it is transcribed; final text locks with a small visual cue; no flickering or text loss occurs.

\textbf{How test will be performed:} Observe display during input; record the session on video; verify that the text updates smoothly and that the finalization cue appears.

\end{enumerate}

\subsubsection{4.1.4 Tests for FR4 — Map Text to Arbitrary Device Commands}



\textbf{Goal:} $\geq$90\% correct mapping on a 50-command test set; handle ambiguity safely.



\begin{enumerate}

\item{test-FR4-MAP-1: Canonical Command Set Accuracy\\}

\textbf{Type:} Automated

\textbf{Initial State:} 50 predefined commands with canonical phrasings and expected command representations (API/CLI/accessibility).

\textbf{Input/Condition:} Exact text phrases.

\textbf{Output/Result:} $\geq$90\% correct mapping to the expected command representation.

\textbf{How test will be performed:} Feed text into intent mapper; compare output to expected representations for accuracy.

\item{test-FR4-MAP-2: Paraphrase \& Synonym Robustness\\}

\textbf{Type:} Automated

\textbf{Initial State:} Same 50 intents with 2--3 paraphrases each.

\textbf{Input/Condition:} Paraphrased commands (e.g., “launch mail app” for “open email”).

\textbf{Output/Result:} Maintains $\geq$90\% accuracy across the combined set or recorded separately for robustness.

\textbf{How test will be performed:} Evaluate precision/recall across the set and calculate percentage.

\item{test-FR4-MAP-3: Ambiguity \& Disambiguation\\}

\textbf{Type:} Manual

\textbf{Initial State:} Commands with ambiguous target (“open Amazon” vs “open Amazon.ca”).

\textbf{Input/Condition:} Ambiguous phrasing.

\textbf{Output/Result:} System asks a brief clarifying question before mapping; no unsafe default.

\textbf{How test will be performed:} Provide ambiguous text; verify clarification prompt and final mapping.

\item{test-FR4-MAP-4: Out-of-Domain Commands\\}

\textbf{Type:} Manual

\textbf{Initial State:} Intent catalog fixed; user requests unsupported app/action.

\textbf{Input/Condition:} “Turn on bedroom lights” (if smart-home unsupported).

\textbf{Output/Result:} Clear feedback: unsupported command + suggestion (link/alternative).

\textbf{How test will be performed:} Issue out-of-domain (OOD) text; verify messaging and no execution.

\end{enumerate}

\subsubsection{4.1.5 Tests for FR5 — Execute Commands on the Host Device}


\textbf{Goal:} For each correctly recognized command, execute within 2 seconds, $\geq$95\% reliability.


\begin{enumerate}

\item{test-FR5-EXEC-1: End-to-End Execution Latency \& Reliability\\}

\textbf{Type:} Automated (instrumented E2E)

\textbf{Initial State:} Known-good mappings for a 50-command set.

\textbf{Input/Condition:} Trigger each command (text or speech $\rightarrow$ text).

\textbf{Output/Result:} Visible system action completes within $\leq$2 s in $\geq$95\% of runs (per command), across platforms.

\textbf{How test will be performed:} Timestamp intent and effect (e.g., window focus/app open), calculate success rate over the 50-command set.

\item{test-FR5-EXEC-2: Accessibility/API Fallback Paths\\}

\textbf{Type:} Manual/Automated

\textbf{Initial State:} Primary API disabled (simulate outage).

\textbf{Input/Condition:} Execute commands that have both accessibility and API paths.

\textbf{Output/Result:} System selects fallback path, still within target latency where feasible, or reports partial degradation.

\textbf{How test will be performed:} Toggle feature flags/mocks; verify fallback invocation.

\item{test-FR5-EXEC-3: Safety \& Confirmation for Destructive Actions\\}

\textbf{Type:} Manual

\textbf{Initial State:} Commands include potentially destructive actions (e.g., “delete email”).

\textbf{Input/Condition:} Issue destructive command.

\textbf{Output/Result:} Confirmation prompt appears; execution only after explicit confirmation.

\textbf{How test will be performed:} Attempt action, verify prompt appears and execution is blocked if canceled.

\end{enumerate}



...

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}