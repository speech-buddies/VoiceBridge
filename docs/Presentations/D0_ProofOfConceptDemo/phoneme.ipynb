{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets jiwer accelerate torchaudio librosa phonemizer editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a011c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "def phoneme_sequence(text):\n",
    "    phones = g2p(text)\n",
    "    return [p for p in phones if p.isalpha()]\n",
    "\n",
    "def phoneme_error_rate(ref, hyp):\n",
    "    ref_ph = phoneme_sequence(ref)\n",
    "    hyp_ph = phoneme_sequence(hyp)\n",
    "\n",
    "    if len(ref_ph) == 0:\n",
    "        return None, [], [], None\n",
    "\n",
    "    dist = editdistance.eval(ref_ph, hyp_ph)\n",
    "    per = dist / len(ref_ph)\n",
    "\n",
    "    return per, ref_ph, hyp_ph, dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ff5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from phonemizer import phonemize\n",
    "from jiwer import cer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "#  PHONEME ERROR RATE\n",
    "# ------------------------\n",
    "def phoneme_sequence(text):\n",
    "    \"\"\"Convert text â†’ list of phonemes.\"\"\"\n",
    "    try:\n",
    "        ph = phonemize(\n",
    "            text,\n",
    "            language=\"en-us\",\n",
    "            backend=\"espeak\",\n",
    "            strip=True,\n",
    "            preserve_punctuation=False,\n",
    "            with_stress=False\n",
    "        )\n",
    "        ph = ph.replace(\" \", \"\")\n",
    "        return list(ph)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def phoneme_error_rate(ref_text, pred_text):\n",
    "    \"\"\"Compute PER and extra phoneme stats.\"\"\"\n",
    "    ref_ph = phoneme_sequence(ref_text)\n",
    "    pred_ph = phoneme_sequence(pred_text)\n",
    "\n",
    "    if len(ref_ph) == 0:\n",
    "        return None, ref_ph, pred_ph, None\n",
    "\n",
    "    ref_str = \"\".join(ref_ph)\n",
    "    pred_str = \"\".join(pred_ph)\n",
    "\n",
    "    # CER works as Levenshtein distance on strings\n",
    "    per = cer(ref_str, pred_str)\n",
    "\n",
    "    # Compute absolute edit distance: PER * length\n",
    "    edit_distance = int(per * len(ref_str))\n",
    "\n",
    "    return per, ref_ph, pred_ph, edit_distance\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "#  MAIN METRIC FUNCTION\n",
    "# ------------------------\n",
    "def transcribe_whisper_and_compute_PER(\n",
    "    main_folder,\n",
    "    model_name=\"openai/whisper-small\",\n",
    "    device=None,\n",
    "    batch_size=4\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "    processor = WhisperProcessor.from_pretrained(model_name)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Loading Whisper model:\", model_name)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    subfolders = [\n",
    "        os.path.join(main_folder, d)\n",
    "        for d in os.listdir(main_folder)\n",
    "        if os.path.isdir(os.path.join(main_folder, d))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(subfolders)} speaker folders.\")\n",
    "\n",
    "    def load_audio(path, target_sr=16000):\n",
    "        audio, sr = sf.read(path)\n",
    "        if sr != target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        return audio\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def batch_transcribe(wav_paths):\n",
    "        outputs = []\n",
    "        for i in tqdm(range(0, len(wav_paths), batch_size)):\n",
    "            batch = [load_audio(p) for p in wav_paths[i:i+batch_size]]\n",
    "            feats = processor(batch, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "            pred_ids = model.generate(input_features=feats)\n",
    "            decoded = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "            outputs.extend([t.strip().lower() for t in decoded])\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    # Loop each speaker folder\n",
    "    for folder in subfolders:\n",
    "        print(f\"\\nProcessing folder: {folder}\")\n",
    "\n",
    "        json_files = glob.glob(os.path.join(folder, \"*.json\"))\n",
    "        if not json_files:\n",
    "            print(\"No JSON file found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_files[0], \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        reference_dict = {\n",
    "            item[\"Filename\"]: item[\"Prompt\"][\"Transcript\"].strip().lower()\n",
    "            for item in data[\"Files\"]\n",
    "        }\n",
    "\n",
    "        wav_files = sorted(glob.glob(os.path.join(folder, \"*.wav\")))\n",
    "        if not wav_files:\n",
    "            print(\"No WAV files found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        preds = batch_transcribe(wav_files)\n",
    "\n",
    "        # Results\n",
    "        for wav_path, pred in zip(wav_files, preds):\n",
    "            fname = os.path.basename(wav_path)\n",
    "            ref = reference_dict.get(fname, \"\")\n",
    "\n",
    "            per, ref_ph, pred_ph, edit_dist = phoneme_error_rate(ref, pred)\n",
    "            phoneme_acc = None if per is None else (1 - per)\n",
    "\n",
    "            # Rating label\n",
    "            def classify(per):\n",
    "                if per is None: return \"No Reference\"\n",
    "                if per < 0.20: return \"Excellent\"\n",
    "                elif per < 0.40: return \"Good\"\n",
    "                elif per < 0.60: return \"Fair\"\n",
    "                else: return \"Poor\"\n",
    "\n",
    "            all_results.append({\n",
    "                \"Folder\": os.path.basename(folder),\n",
    "                \"Filename\": fname,\n",
    "                \"Reference\": ref,\n",
    "                \"Prediction\": pred,\n",
    "                \"PER\": per,\n",
    "                \"PhonemeAccuracy\": phoneme_acc,\n",
    "                \"EditDistance\": edit_dist,\n",
    "                \"RefLength\": len(ref_ph),\n",
    "                \"PredLength\": len(pred_ph),\n",
    "                \"Rating\": classify(per),\n",
    "                \"RefPhonemes\": \" \".join(ref_ph),\n",
    "                \"PredPhonemes\": \" \".join(pred_ph)\n",
    "            })\n",
    "\n",
    "    # ------------------------\n",
    "    # FINAL DATAFRAME\n",
    "    # ------------------------\n",
    "    df = pd.DataFrame(all_results)\n",
    "\n",
    "    # ------------------------\n",
    "    # GLOBAL STATS\n",
    "    # ------------------------\n",
    "    print(\"\\n================ GLOBAL PER STATISTICS ================\\n\")\n",
    "    print(df[\"PER\"].describe())\n",
    "    print(f\"\\nMedian PER: {df['PER'].median():.3f}\")\n",
    "    print(f\"Best PER (min): {df['PER'].min():.3f}\")\n",
    "    print(f\"Worst PER (max): {df['PER'].max():.3f}\")\n",
    "\n",
    "    # ------------------------\n",
    "    # FOLDER SUMMARY\n",
    "    # ------------------------\n",
    "    print(\"\\n================ PER-FOLDER SUMMARY =================\\n\")\n",
    "    folder_stats = df.groupby(\"Folder\").agg(\n",
    "        Mean_PER=(\"PER\", \"mean\"),\n",
    "        Median_PER=(\"PER\", \"median\"),\n",
    "        Std_PER=(\"PER\", \"std\"),\n",
    "        Min_PER=(\"PER\", \"min\"),\n",
    "        Max_PER=(\"PER\", \"max\"),\n",
    "        Samples=(\"PER\", \"count\")\n",
    "    )\n",
    "    print(folder_stats)\n",
    "\n",
    "    # ------------------------\n",
    "    # WORST SAMPLES\n",
    "    # ------------------------\n",
    "    print(\"\\n================ WORST SAMPLES (HIGH PER) =================\\n\")\n",
    "    print(df.sort_values(\"PER\", ascending=False).head(15).to_string(index=False))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transcribe_whisper_and_compute_PER(\"./test_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"full_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
