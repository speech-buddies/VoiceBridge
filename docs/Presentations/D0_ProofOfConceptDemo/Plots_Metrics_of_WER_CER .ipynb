{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuPtI8mguqCF"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate soundfile librosa pandas tqdm torch jiwer peft seaborn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txR1elVp0KGc",
        "outputId": "aee11b15-2ac1-4214-f70d-8200dce756d5"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "from jiwer import wer, cer\n",
        "\n",
        "# Google Drive mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load audio into Whisper input features\n",
        "def load_audio_for_whisper(path, processor):\n",
        "    audio, sr = librosa.load(path, sr=16000)\n",
        "    return processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "\n",
        "# Path to held out + rated samples\n",
        "dataset_folder = \"/content/drive/My Drive/capstone/held_out_data_with_ratings\"\n",
        "\n",
        "# LoRA checkpoint path\n",
        "lora_checkpoint = \"/content/drive/MyDrive/whisper_lora_epoch1.pt\"\n",
        "\n",
        "# Notebook ID for results\n",
        "NOTEBOOK_ID = \"1xm-qp_aGvQw0vkTj9wCq4spsoeI8LYlL\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neEDoD4g84QD",
        "outputId": "83f8662a-ad2a-45a9-c4fc-458af26f946a"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Base model\n",
        "base_model_name = \"openai/whisper-small\"\n",
        "base_model = WhisperForConditionalGeneration.from_pretrained(base_model_name)\n",
        "processor = WhisperProcessor.from_pretrained(base_model_name)\n",
        "\n",
        "# Load LoRA checkpoint\n",
        "state_dict = torch.load(lora_checkpoint, map_location=\"cpu\")\n",
        "vocab_ckpt = state_dict['base_model.model.model.decoder.embed_tokens.weight'].shape[0]\n",
        "vocab_base = base_model.config.vocab_size\n",
        "\n",
        "# Resize embeddings BEFORE PEFT\n",
        "if vocab_base != vocab_ckpt:\n",
        "    base_model.model.decoder.embed_tokens = torch.nn.Embedding(vocab_ckpt, base_model.config.d_model)\n",
        "    base_model.model.proj_out = torch.nn.Linear(base_model.config.d_model, vocab_ckpt)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "lora_model = get_peft_model(base_model, peft_config)\n",
        "\n",
        "lora_model.load_state_dict(state_dict, strict=False)\n",
        "lora_model = lora_model.to(DEVICE)\n",
        "lora_model.eval()\n",
        "print(\"LoRA model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_OPuWQV56Uy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_metrics_whisper(dataset_folder, model=None, processor=None, model_name_or_path=None):\n",
        "    \"\"\"\n",
        "    Only the first disease per file is considered, and patient ID is extracted\n",
        "    from the filename.\n",
        "    Saves a JSON file with results using the notebook ID.\n",
        "\n",
        "    model: optional, pre-loaded Whisper model (e.g., LoRA-attached)\n",
        "    processor: optional, corresponding WhisperProcessor\n",
        "    model_name_or_path: str, used if model/processor not provided\n",
        "    \"\"\"\n",
        "    NOTEBOOK_ID = \"1xm-qp_aGvQw0vkTj9wCq4spsoeI8LYlL\"\n",
        "    BATCH_SIZE = 11\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if model is None or processor is None:\n",
        "        if model_name_or_path is None:\n",
        "            raise ValueError(\"Either model/processor or model_name_or_path must be provided.\")\n",
        "        print(\"Using device:\", DEVICE)\n",
        "        print(\"Using model/path:\", model_name_or_path)\n",
        "        processor = WhisperProcessor.from_pretrained(model_name_or_path)\n",
        "        model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path).to(DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Load metadata\n",
        "    metadata_path = os.path.join(dataset_folder, \"held_out_data_with_ratings_metadata.json\")\n",
        "\n",
        "    with open(metadata_path, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    audio_files = sorted([os.path.join(dataset_folder, f)\n",
        "                          for f in os.listdir(dataset_folder)\n",
        "                          if f.lower().endswith(\".wav\")])\n",
        "    if not audio_files:\n",
        "        raise ValueError(\"No WAV files found in dataset folder!\")\n",
        "\n",
        "    wer_metric = evaluate.load(\"wer\")\n",
        "    cer_metric = evaluate.load(\"cer\")\n",
        "    results = []\n",
        "\n",
        "    for i in tqdm(range(0, len(audio_files), BATCH_SIZE)):\n",
        "        batch_files = audio_files[i:i+BATCH_SIZE]\n",
        "        batch_inputs = []\n",
        "\n",
        "        for path in batch_files:\n",
        "            audio, sr = librosa.load(path, sr=16000)\n",
        "            feats = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "            batch_inputs.append(feats)\n",
        "\n",
        "        batch_inputs = torch.cat(batch_inputs).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_ids = model.generate(inputs=batch_inputs)\n",
        "        batch_texts = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "        for path, pred_text in zip(batch_files, batch_texts):\n",
        "            filename = os.path.basename(path)\n",
        "\n",
        "\n",
        "            meta_entry = next((m for m in metadata if m.get(\"Filename\") == filename), None)\n",
        "            if meta_entry is None:\n",
        "                raise KeyError(f\"No metadata entry found for {filename}\")\n",
        "\n",
        "            ref_text = meta_entry[\"Prompt\"][\"Transcript\"].strip().lower()\n",
        "            file_wer = wer_metric.compute(predictions=[pred_text.lower()], references=[ref_text])\n",
        "            file_cer = cer_metric.compute(predictions=[pred_text.lower()], references=[ref_text])\n",
        "\n",
        "            ratings = meta_entry.get(\"Ratings\", [])\n",
        "\n",
        "            symptoms_dict = {r[\"Dimension Description\"]: r[\"Level\"] for r in ratings if r.get(\"Dimension Category Description\")}\n",
        "            first_disease = ratings[0][\"Dimension Category Description\"] if ratings and ratings[0].get(\"Dimension Category Description\") else None\n",
        "            patient_id = filename.split(\"-\")[0]\n",
        "\n",
        "            results.append({\n",
        "                \"Filename\": filename,\n",
        "                \"Patient_ID\": patient_id,\n",
        "                \"Disease\": first_disease,\n",
        "                \"Symptoms\": symptoms_dict,\n",
        "                \"Reference\": ref_text,\n",
        "                \"Prediction\": pred_text,\n",
        "                \"WER\": file_wer,\n",
        "                \"CER\": file_cer\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    overall_wer = wer_metric.compute(predictions=df[\"Prediction\"].str.lower(), references=df[\"Reference\"].str.lower())\n",
        "    overall_cer = cer_metric.compute(predictions=df[\"Prediction\"].str.lower(), references=df[\"Reference\"].str.lower())\n",
        "    print(\"\\n==== Overall Metrics ====\")\n",
        "    print(\"Overall WER:\", overall_wer)\n",
        "    print(\"Overall CER:\", overall_cer)\n",
        "\n",
        "    save_path = os.path.join(dataset_folder, f\"metrics_results_{NOTEBOOK_ID}_{os.path.basename(model_name_or_path) if model_name_or_path else 'custom_model'}.json\")\n",
        "    df.to_json(save_path, orient=\"records\", indent=4)\n",
        "    print(f\"Saved results â†’ {save_path}\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfsVo04P58l2"
      },
      "outputs": [],
      "source": [
        "def plot_results(df, top_symptoms=10):\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # WER & CER distributions\n",
        "    plt.figure(figsize=(12,4))\n",
        "    sns.histplot(df[\"WER\"], bins=20, kde=True, color=\"skyblue\")\n",
        "    plt.title(\"Distribution of WER\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12,4))\n",
        "    sns.histplot(df[\"CER\"], bins=20, kde=True, color=\"salmon\")\n",
        "    plt.title(\"Distribution of CER\")\n",
        "    plt.show()\n",
        "\n",
        "    # WER/CER by Disease\n",
        "    if df[\"Disease\"].nunique() > 0:\n",
        "        plt.figure(figsize=(10,4))\n",
        "        sns.barplot(x=\"Disease\", y=\"WER\", data=df)\n",
        "        plt.title(\"Average WER by Disease\")\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10,4))\n",
        "        sns.barplot(x=\"Disease\", y=\"CER\", data=df)\n",
        "        plt.title(\"Average CER by Disease\")\n",
        "        plt.show()\n",
        "\n",
        "    # WER by Patient\n",
        "    plt.figure(figsize=(14,4))\n",
        "    sns.barplot(x=\"Patient_ID\", y=\"WER\", data=df)\n",
        "    plt.title(\"WER per Patient\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "    # Symptoms vs WER (top N for readability)\n",
        "    all_symptoms = []\n",
        "    for idx, row in df.iterrows():\n",
        "        for sym, lvl in row[\"Symptoms\"].items():\n",
        "            all_symptoms.append({\"Symptom\": sym, \"Level\": lvl, \"WER\": row[\"WER\"], \"CER\": row[\"CER\"]})\n",
        "\n",
        "    if all_symptoms:\n",
        "        sym_df = pd.DataFrame(all_symptoms)\n",
        "        top_symptom_names = sym_df[\"Symptom\"].value_counts().head(top_symptoms).index\n",
        "        sym_df_top = sym_df[sym_df[\"Symptom\"].isin(top_symptom_names)]\n",
        "\n",
        "        plt.figure(figsize=(12,5))\n",
        "        sns.boxplot(x=\"Symptom\", y=\"WER\", hue=\"Level\", data=sym_df_top)\n",
        "        plt.title(f\"WER by Top {top_symptoms} Symptoms and Level\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NeDHHX76BDY",
        "outputId": "225eed50-5880-4212-88fc-452a3271a327"
      },
      "outputs": [],
      "source": [
        "dataset_folder = \"/content/drive/My Drive/capstone/held_out_data_with_ratings\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Base\n",
        "base_model_name = \"openai/whisper-small\"\n",
        "df_base = compute_metrics_whisper(dataset_folder, model_name_or_path=base_model_name)\n",
        "\n",
        "# LoRA tuned model\n",
        "#  Load base model & processor\n",
        "base_model = WhisperForConditionalGeneration.from_pretrained(base_model_name)\n",
        "processor = WhisperProcessor.from_pretrained(base_model_name)\n",
        "\n",
        "# Attach LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "lora_model = get_peft_model(base_model, peft_config)\n",
        "\n",
        "# Load LoRA checkpoint weights\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/whisper_lora_epoch1.pt\", map_location=\"cpu\")\n",
        "lora_model.load_state_dict(state_dict, strict=False)\n",
        "lora_model.to(DEVICE)\n",
        "lora_model.eval()\n",
        "\n",
        "# Compute metrics using the loaded LoRA model\n",
        "df_lora = compute_metrics_whisper(dataset_folder, model=lora_model, processor=processor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "liBVzHrNBWEE",
        "outputId": "127cf396-7825-4010-85cd-f8bc98fe5097"
      },
      "outputs": [],
      "source": [
        "# For base model\n",
        "plot_results(df_base)\n",
        "\n",
        "# For LoRA model\n",
        "plot_results(df_lora)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_Ke3kzNgdSa"
      },
      "outputs": [],
      "source": [
        "def plot_symptom_histograms(df):\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Flatten all symptoms\n",
        "    all_symptoms = []\n",
        "    for idx, row in df.iterrows():\n",
        "        for sym, lvl in row[\"Symptoms\"].items():\n",
        "            all_symptoms.append({\"Symptom\": sym, \"Level\": lvl, \"WER\": row[\"WER\"]})\n",
        "\n",
        "    if not all_symptoms:\n",
        "        print(\"No symptoms found in dataset!\")\n",
        "        return\n",
        "\n",
        "    sym_df = pd.DataFrame(all_symptoms)\n",
        "    unique_symptoms = sym_df[\"Symptom\"].unique()\n",
        "\n",
        "    for symptom in unique_symptoms:\n",
        "        symptom_data = sym_df[sym_df[\"Symptom\"] == symptom]\n",
        "\n",
        "        plt.figure(figsize=(8,5))\n",
        "        levels = symptom_data[\"Level\"].unique()\n",
        "        for lvl in levels:\n",
        "            lvl_data = symptom_data[symptom_data[\"Level\"] == lvl][\"WER\"]\n",
        "            plt.hist(lvl_data, bins=10, alpha=0.6, label=f\"Level {lvl}\")\n",
        "\n",
        "        plt.title(f\"WER Histogram for Symptom: {symptom}\")\n",
        "        plt.xlabel(\"WER\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ingn9fMbhBxe",
        "outputId": "0a7f5289-a76f-4a53-cc4d-1a10e73f3ae3"
      },
      "outputs": [],
      "source": [
        "# For the base model\n",
        "plot_symptom_histograms(df_base)\n",
        "\n",
        "# For the LoRA-tuned model\n",
        "plot_symptom_histograms(df_lora)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
