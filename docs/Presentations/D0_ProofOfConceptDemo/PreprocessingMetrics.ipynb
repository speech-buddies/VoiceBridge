{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4EV81ZpPLUg",
        "outputId": "5508955c-862e-46ac-e00e-feedde1da4f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "google-adk 1.19.0 requires sqlalchemy<3.0.0,>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install torch torchaudio transformers datasets peft bitsandbytes accelerate torchcodec dataset evaluate bitsandbytes jiwer -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0F_4DtuVYWb"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN=\"xxx\"  # --- Add Hugging Face Token ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udr9VP2AZJVk",
        "outputId": "56e14704-238f-4688-a106-df9a92e5f6b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "\n",
        "model_name = \"openai/whisper-small.en\"\n",
        "processor = WhisperProcessor.from_pretrained(model_name)  # (same as original, reused everywhere)\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training  # (original import)\n",
        "\n",
        "# CHANGED: instead of creating a single global model once,\n",
        "# we wrap the original QLoRA setup into a helper so we can\n",
        "# create a fresh model *per dataset* (regular vs trimmed).\n",
        "def create_lora_model():\n",
        "    \"\"\"Create a fresh 4-bit QLoRA Whisper model (same config as original).\"\"\"\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,               # quantize base model\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    # For QLoRA (same as original)\n",
        "    model = prepare_model_for_kbit_training(model)  # enables 4-bit quantization\n",
        "\n",
        "    config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],  # or attention layers depending on model\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"SEQ_2_SEQ_LM\"  # or \"CTC\" for wav2vec2\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # CHANGED: the forward patch originally lived at global scope;\n",
        "    # now we apply it inside this helper so each new model is patched.\n",
        "    from types import MethodType\n",
        "\n",
        "    def patched_forward(self, input_features=None, **kwargs):\n",
        "        return self.model.forward(input_features=input_features, **kwargs)\n",
        "\n",
        "    model.forward = MethodType(patched_forward, model)  # CHANGED: moved from global into helper\n",
        "\n",
        "    return model  # NEW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yszn0KscZZKT",
        "outputId": "15c8f916-a64d-4bfd-b2bd-22b6845fbdd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, json\n",
        "from datasets import Dataset, Audio\n",
        "\n",
        "# ---- CONFIG ----\n",
        "SAMPLING_RATE = 16_000  # set based on your ASR model\n",
        "\n",
        "# NEW: two dataset directories instead of one, as per your requirement.\n",
        "DATA_DIR_REGULAR = '/content/drive/My Drive/VoiceBridge_SAP_sample'          # CHANGED: was VoiceBridge_SAP_sample\n",
        "DATA_DIR_TRIMMED = '/content/drive/My Drive/VoiceBridge_SAP_sample_trimmed'  # NEW\n",
        "META_FILENAME = \"digital_assistant_metadata.json\"  # NEW: common metadata filename\n",
        "\n",
        "\n",
        "# NEW: factor out the original metadata + dataset-building logic so we can reuse it.\n",
        "def build_dataset_from_dir(data_dir: str):\n",
        "    \"\"\"\n",
        "    Load metadata and build a HuggingFace Dataset from a given directory.\n",
        "    This is the same logic as your original code, but parameterized by `data_dir`.\n",
        "    \"\"\"\n",
        "    meta_file = os.path.join(data_dir, META_FILENAME)  # CHANGED: use parameterized dir\n",
        "\n",
        "    # ---- LOAD METADATA ----\n",
        "    with open(meta_file, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # ---- BUILD LIST OF AUDIO–TEXT PAIRS ----\n",
        "    data = []\n",
        "    for item in metadata:\n",
        "        filename = item.get(\"Filename\")\n",
        "        transcript = item.get(\"Prompt\", {}).get(\"Transcript\", \"\")\n",
        "        if not filename or not transcript:\n",
        "            continue\n",
        "        audio_path = os.path.join(data_dir, filename)  # CHANGED: was DATA_DIR\n",
        "        if os.path.exists(audio_path):\n",
        "            data.append({\"audio\": audio_path, \"sentence\": transcript})\n",
        "\n",
        "    print(f\"[{data_dir}] Found {len(data)} usable audio–transcript pairs\")\n",
        "\n",
        "    # ---- CREATE HUGGING FACE DATASET ----\n",
        "    dataset = Dataset.from_list(data)\n",
        "    # dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
        "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE, decode=False))\n",
        "    return dataset  # NEW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwIVPoYs00yE"
      },
      "outputs": [],
      "source": [
        "# (kept from original)\n",
        "from transformers import WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small.en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mijr0trY0389"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "\n",
        "\n",
        "def preprocess(batch):\n",
        "    speech_arrays = []\n",
        "    for audio_info in batch[\"audio\"]:\n",
        "        path = audio_info[\"path\"]\n",
        "        speech_array, sr = torchaudio.load(path)\n",
        "\n",
        "        # CHANGED: always downmix to mono so every sample is 1D\n",
        "        if speech_array.ndim == 2:               # (channels, samples)\n",
        "            # average across channels -> mono\n",
        "            speech_array = speech_array.mean(dim=0)\n",
        "\n",
        "        # CHANGED: ensure we end up with a 1D numpy array\n",
        "        speech_arrays.append(speech_array.numpy())\n",
        "\n",
        "    # Feature extraction: do NOT use return_tensors here\n",
        "    input_features = processor.feature_extractor(\n",
        "        speech_arrays,\n",
        "        sampling_rate=SAMPLING_RATE,\n",
        "    ).input_features  # list of arrays\n",
        "\n",
        "    # Tokenize transcripts (unchanged)\n",
        "    labels = processor.tokenizer(\n",
        "        batch[\"sentence\"],\n",
        "        add_special_tokens=True\n",
        "    ).input_ids\n",
        "\n",
        "    return {\"input_features\": input_features, \"labels\": labels}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0-3ccO11B9W"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
        "        labels_batch = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
        "\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "        labels = self.processor.tokenizer.pad(labels_batch, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "        batch[\"labels\"] = labels.masked_fill(labels == self.processor.tokenizer.pad_token_id, -100)\n",
        "        return batch\n",
        "\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UElSDaqi1RR2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zhqyBAE1XCl"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "# NEW: wrapper to do the *entire* training pipeline for a given folder.\n",
        "def train_q_lora_for_folder(data_dir: str, output_dir_root: str):\n",
        "    \"\"\"\n",
        "    Train a QLoRA Whisper model on the dataset in `data_dir`.\n",
        "    This wraps your original \"dataset + trainer + train + save\" code so\n",
        "    we can call it for both regular and trimmed folders.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Training QLoRA model for: {data_dir} ===\")\n",
        "\n",
        "    # 1) Build dataset (original logic, now via helper)\n",
        "    dataset = build_dataset_from_dir(data_dir)  # NEW\n",
        "\n",
        "    # 2) Map preprocess (same as original)\n",
        "    dataset = dataset.map(\n",
        "        preprocess,\n",
        "        # remove_columns=[\"audio\", \"sentence\"],\n",
        "        batched=True,\n",
        "        batch_size=4,\n",
        "        num_proc=1,\n",
        "    )\n",
        "\n",
        "    # 3) Split train / eval (same as original)\n",
        "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    train_dataset = dataset[\"train\"]\n",
        "    eval_dataset = dataset[\"test\"]\n",
        "\n",
        "    # 4) Create a fresh LoRA model (NEW vs original single global model)\n",
        "    model = create_lora_model()  # NEW\n",
        "\n",
        "    # 5) Optional: quick forward pass check (same logic as original)\n",
        "    batch = data_collator([train_dataset[0], train_dataset[1]])\n",
        "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "\n",
        "    outputs = model(\n",
        "        input_features=batch[\"input_features\"],\n",
        "        labels=batch[\"labels\"]\n",
        "    )\n",
        "    print(\"Sanity check loss:\", outputs.loss.item())\n",
        "\n",
        "    # 6) Training arguments (same as original, but output_dir is parameterized)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir_root,          # CHANGED: was \"./whisper-lora-checkpoints\"\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=1e-4,\n",
        "        num_train_epochs=3,\n",
        "        fp16=False,\n",
        "        bf16=False,\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        report_to=\"none\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    # 7) Trainer (same as original)\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=None,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # 8) Train (same as original)\n",
        "    trainer.train()\n",
        "\n",
        "    # 9) Save adapters and merged model (same logic, but under output_dir_root)\n",
        "    adapter_dir = os.path.join(output_dir_root, \"whisper-lora-adapter\")   # NEW\n",
        "    merged_dir = os.path.join(output_dir_root, \"whisper-lora-merged\")     # NEW\n",
        "\n",
        "    model.save_pretrained(adapter_dir)\n",
        "\n",
        "    merged_model = model.merge_and_unload()\n",
        "    merged_model.save_pretrained(merged_dir)\n",
        "\n",
        "    processor.save_pretrained(merged_dir)\n",
        "\n",
        "    print(f\"Saved adapter to: {adapter_dir}\")\n",
        "    print(f\"Saved merged model to: {merged_dir}\")\n",
        "\n",
        "    # Return merged_model so we can evaluate WER on it\n",
        "    return merged_model, merged_dir, eval_dataset  # NEW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509,
          "referenced_widgets": [
            "e7b154ac781a4c8ca24847c668f51bd6",
            "0407a65d54c14f6aa76ebfa46d4f052d",
            "4405237b613d4221a0b47eef7fc2fd78",
            "cf0b835a58b44d7fb9f3742f628a7af2",
            "22a89e60bd58475096d94b7e1d7391f3",
            "99b34de8d55e4a668d0e7cda6c660a5e",
            "6718a72ebfaf4acab58b2db1a0d3b039",
            "4f39fabfc6034c558cbcf0f7c6f23540",
            "2cd8645da7c646f385cfe539574e3784",
            "8756b02425aa493494ad57b9ec17e3d4",
            "b492604d80d14d44ab58b2098364da15"
          ]
        },
        "id": "aov7100Z1o0l",
        "outputId": "4d6f47c1-693f-4eb3-f578-9cbee6815387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training QLoRA model for: /content/drive/My Drive/VoiceBridge_SAP_sample ===\n",
            "[/content/drive/My Drive/VoiceBridge_SAP_sample] Found 207 usable audio–transcript pairs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7b154ac781a4c8ca24847c668f51bd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,769,472 || all params: 243,503,616 || trainable%: 0.7267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity check loss: 7.226772785186768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-506667662.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [72/72 2:00:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapter to: /content/drive/My Drive/whisper-lora-regular-checkpoints/whisper-lora-adapter\n",
            "Saved merged model to: /content/drive/My Drive/whisper-lora-regular-checkpoints/whisper-lora-merged\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR_REGULAR = \"/content/drive/My Drive/whisper-lora-regular-checkpoints\"\n",
        "\n",
        "model_regular, merged_dir_regular, eval_dataset_regular = train_q_lora_for_folder(\n",
        "    DATA_DIR_REGULAR,\n",
        "    OUTPUT_DIR_REGULAR,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "3c204926838b4863b5d92ebc49a98eb7",
            "f62841ba2feb4f4d8f2b4100a085b197",
            "cc18d01b14764745ac9acfaac9945e79",
            "55fb8af903d343578044b6f362502498",
            "7f1a50638759468dbda943940e207e13",
            "bcb4568c50d64caebe9299eca93d6a9d",
            "80af569074d74c7380385b83e36cc965",
            "6ef9474a0c984c3c8db93e2d6d17a634",
            "7cfa0779a3d84867956464425d5572b5",
            "e74f993a9a4549eeb41eba3859d6ac96",
            "7e446f565ecc4871bbdbba1048de6fe2"
          ]
        },
        "id": "QveVxMWmmQ67",
        "outputId": "2295cf94-ecd1-4270-ed94-f631527a98ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training QLoRA model for: /content/drive/My Drive/VoiceBridge_SAP_sample_trimmed ===\n",
            "[/content/drive/My Drive/VoiceBridge_SAP_sample_trimmed] Found 207 usable audio–transcript pairs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c204926838b4863b5d92ebc49a98eb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,769,472 || all params: 243,503,616 || trainable%: 0.7267\n",
            "Sanity check loss: 7.507758617401123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-506667662.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [72/72 2:00:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapter to: /content/drive/My Drive/whisper-lora-trimmed-checkpoints/whisper-lora-adapter\n",
            "Saved merged model to: /content/drive/My Drive/whisper-lora-trimmed-checkpoints/whisper-lora-merged\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR_TRIMMED = \"/content/drive/My Drive/whisper-lora-trimmed-checkpoints\"\n",
        "\n",
        "model_trimmed, merged_dir_trimmed, eval_dataset_trimmed = train_q_lora_for_folder(\n",
        "    DATA_DIR_TRIMMED,\n",
        "    OUTPUT_DIR_TRIMMED,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiznrUloeF0U",
        "outputId": "351d8910-8f4a-40b7-d65c-c5baafa5a364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q evaluate jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K_q_l_Pg6J1",
        "outputId": "a595bd96-89e1-4050-f6b3-21cc18d8905a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAGixI1l1_Qa",
        "outputId": "5bd6b0c3-42b1-4b2d-a6b7-f40c58e9453d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[MERGE] From adapter dir: /content/drive/My Drive/whisper-lora-regular-checkpoints/whisper-lora-adapter\n",
            "[MERGE] To merged dir:    /content/drive/My Drive/whisper-lora-regular-checkpoints/whisper-lora-merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[MERGE] Done.\n",
            "\n",
            "[MERGE] From adapter dir: /content/drive/My Drive/whisper-lora-trimmed-checkpoints/whisper-lora-adapter\n",
            "[MERGE] To merged dir:    /content/drive/My Drive/whisper-lora-trimmed-checkpoints/whisper-lora-merged\n",
            "[MERGE] Done.\n",
            "\n",
            "[MERGE] Finished merging both models.\n",
            "\n",
            "Using device: cuda\n",
            "[/content/drive/My Drive/VoiceBridge_SAP_sample] Found 7431 reference entries in JSON files\n",
            "[/content/drive/My Drive/VoiceBridge_SAP_sample] Found 207 wav files\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTranscribing (VoiceBridge_SAP_sample):   0%|          | 0/52 [00:00<?, ?it/s]A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n",
            "Transcribing (VoiceBridge_SAP_sample): 100%|██████████| 52/52 [02:52<00:00,  3.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[VoiceBridge_SAP_sample] Overall average WER: 0.757\n",
            "\n",
            "Top 10 highest WER samples:\n",
            "                                            Filename  WER          Reference                       Prediction\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29119_13128.wav  5.0      escitalopram.       and please stay long. bye.\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29338_13367.wav  4.0           avocado.                  i will call on.\n",
            "23b27207-6d7f-4da9-a85b-08dcf77c2ab9_29853_13492.wav  4.0         audiobook.               how will you book?\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29122_13127.wav  3.0      take focalin.  they thought they would use it.\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29111_12390.wav  3.0           aricept.              bye, little one-fa.\n",
            "  2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_148_13127.wav  2.0           cortana.                        good job.\n",
            "  2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_151_12390.wav  2.0            redial.                      with the...\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29226_12390.wav  2.0            refill.                           with a\n",
            "   2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_69_12390.wav  2.0 turn off switches. it's all right, let's finish it.\n",
            "  2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_217_12390.wav  1.5   costco shopping.                  that's the job.\n",
            "[/content/drive/My Drive/VoiceBridge_SAP_sample_trimmed] Found 7431 reference entries in JSON files\n",
            "[/content/drive/My Drive/VoiceBridge_SAP_sample_trimmed] Found 207 wav files\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing (VoiceBridge_SAP_sample_trimmed): 100%|██████████| 52/52 [02:14<00:00,  2.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[VoiceBridge_SAP_sample_trimmed] Overall average WER: 0.819\n",
            "\n",
            "Top 10 highest WER samples:\n",
            "                                            Filename  WER        Reference            Prediction\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29119_13128.wav  5.0    escitalopram. at the state of life.\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29338_13367.wav  4.0         avocado.      i will carry on.\n",
            "    0c3a355b-c4da-40fe-57de-08dcbb9f64e1_29_8160.wav  2.0         cortana.          from talent.\n",
            "23b27207-6d7f-4da9-a85b-08dcf77c2ab9_29853_13492.wav  2.0       audiobook.             aria book\n",
            "23b27207-6d7f-4da9-a85b-08dcf77c2ab9_29349_13492.wav  2.0        arriving.              bye bye.\n",
            "  2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_148_13127.wav  2.0         cortana.           with color.\n",
            "  28fe23ad-6c0d-411b-03f3-08dd318df37f_148_13474.wav  2.0         cortana.            good time.\n",
            "  f6616ff1-176f-477d-1cda-08dce2a2fe9b_151_13018.wav  2.0          redial.              read out\n",
            "  2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_217_12390.wav  2.0 costco shopping.   that's the job pay.\n",
            "   2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_35_13127.wav  2.0           email.                 in my\n",
            "[/content/drive/My Drive/VoiceBridge_SAP_sample] Found 7431 reference entries in JSON files\n",
            "[/content/drive/My Drive/VoiceBridge_SAP_sample] Found 207 wav files\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing (VoiceBridge_SAP_sample): 100%|██████████| 52/52 [00:09<00:00,  5.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[VoiceBridge_SAP_sample] Overall average WER: 0.779\n",
            "\n",
            "Top 10 highest WER samples:\n",
            "                                            Filename      WER              Reference                       Prediction\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29119_13128.wav 6.000000          escitalopram.    and please stay in our place.\n",
            "23b27207-6d7f-4da9-a85b-08dcf77c2ab9_29853_13492.wav 4.000000             audiobook.              how we have worked?\n",
            "23b27207-6d7f-4da9-a85b-08dcf77c2ab9_29108_13492.wav 3.000000              adderall?                     up the ball.\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29122_13127.wav 3.000000          take focalin.  they thought they would use it.\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29338_13367.wav 3.000000               avocado.                    i will cover.\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29111_12390.wav 2.000000               aricept.                     bye, loonfa.\n",
            "  2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_148_13127.wav 2.000000               cortana.                        good job.\n",
            "   2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_69_12390.wav 2.000000     turn off switches. it's all right, let's finish it.\n",
            "    0c3a355b-c4da-40fe-57de-08dcbb9f64e1_24_8267.wav 1.666667       one seven seven.       we will save our sandwich.\n",
            "2dcb3ebc-59ea-4275-75a6-08dd3ba78fcd_29358_13127.wav 1.666667 call herman gutierrez.            so how much is there?\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Merge LoRA checkpoints and test the models\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from peft import PeftModel\n",
        "import evaluate\n",
        "\n",
        "# --- paths (must match what you used in training) ---\n",
        "DATA_DIR_REGULAR = \"/content/drive/My Drive/VoiceBridge_SAP_sample\"\n",
        "DATA_DIR_TRIMMED = \"/content/drive/My Drive/VoiceBridge_SAP_sample_trimmed\"\n",
        "\n",
        "# These are the *parent* dirs where training outputs live\n",
        "OUTPUT_DIR_REGULAR = \"/content/drive/My Drive/whisper-lora-regular-checkpoints\"\n",
        "OUTPUT_DIR_TRIMMED = \"/content/drive/My Drive/whisper-lora-trimmed-checkpoints\"\n",
        "\n",
        "# These are the LoRA adapter dirs (where adapter_config.json actually is)\n",
        "ADAPTER_DIR_REGULAR = os.path.join(OUTPUT_DIR_REGULAR, \"whisper-lora-adapter\")\n",
        "ADAPTER_DIR_TRIMMED = os.path.join(OUTPUT_DIR_TRIMMED, \"whisper-lora-adapter\")\n",
        "\n",
        "# Where we'll save merged full models\n",
        "MERGED_DIR_REGULAR = os.path.join(OUTPUT_DIR_REGULAR, \"whisper-lora-merged\")\n",
        "MERGED_DIR_TRIMMED = os.path.join(OUTPUT_DIR_TRIMMED, \"whisper-lora-merged\")\n",
        "\n",
        "BASE_MODEL_ID = \"openai/whisper-small.en\"\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1) Merge LoRA adapters into full Whisper models\n",
        "# ----------------------------------------------------\n",
        "\n",
        "processor_base = WhisperProcessor.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "def merge_lora_checkpoint(adapter_dir: str, merged_dir: str):\n",
        "    \"\"\"\n",
        "    Load base Whisper, attach LoRA from `adapter_dir`, merge, and save\n",
        "    a normal (non-quantized) model into `merged_dir`.\n",
        "    \"\"\"\n",
        "    print(f\"\\n[MERGE] From adapter dir: {adapter_dir}\")\n",
        "    print(f\"[MERGE] To merged dir:    {merged_dir}\")\n",
        "\n",
        "    if not os.path.isdir(adapter_dir):\n",
        "        raise FileNotFoundError(f\"Adapter dir not found: {adapter_dir}\")\n",
        "\n",
        "    # Load base model in full precision (no bitsandbytes)\n",
        "    base_model = WhisperForConditionalGeneration.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        torch_dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "    # Attach LoRA weights from adapter directory\n",
        "    lora_model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
        "\n",
        "    # Merge LoRA into the base weights\n",
        "    lora_model = lora_model.merge_and_unload()\n",
        "\n",
        "    # Save merged model + processor\n",
        "    os.makedirs(merged_dir, exist_ok=True)\n",
        "    lora_model.to(\"cpu\")\n",
        "    lora_model.save_pretrained(merged_dir)\n",
        "    processor_base.save_pretrained(merged_dir)\n",
        "\n",
        "    print(\"[MERGE] Done.\")\n",
        "\n",
        "# Merge both REGULAR and TRIMMED LoRA checkpoints\n",
        "merge_lora_checkpoint(ADAPTER_DIR_REGULAR, MERGED_DIR_REGULAR)\n",
        "merge_lora_checkpoint(ADAPTER_DIR_TRIMMED, MERGED_DIR_TRIMMED)\n",
        "\n",
        "print(\"\\n[MERGE] Finished merging both models.\\n\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2) Load merged models (no quantization, no bitsandbytes)\n",
        "# ----------------------------------------------------\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model_regular = WhisperForConditionalGeneration.from_pretrained(\n",
        "    MERGED_DIR_REGULAR,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n",
        "model_regular.to(device)\n",
        "\n",
        "model_trimmed = WhisperForConditionalGeneration.from_pretrained(\n",
        "    MERGED_DIR_TRIMMED,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n",
        "model_trimmed.to(device)\n",
        "\n",
        "baseline_model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n",
        "baseline_model.to(device)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3) WER function for Whisper (nested folder layout)\n",
        "# ----------------------------------------------------\n",
        "\n",
        "def transcribe_whisper_and_compute_wer(\n",
        "    root_folder,\n",
        "    model,\n",
        "    device=None,\n",
        "    batch_size=4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Walk `root_folder`, find all .json files, build:\n",
        "        abs wav path -> transcript\n",
        "    Then transcribe all wavs and compute WER per file.\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1) Build reference dict\n",
        "    reference_dict = {}\n",
        "    for root, _, files in os.walk(root_folder):\n",
        "        for fname in files:\n",
        "            if fname.lower().endswith(\".json\"):\n",
        "                json_path = os.path.join(root, fname)\n",
        "                with open(json_path, \"r\") as f:\n",
        "                    metadata = json.load(f)\n",
        "\n",
        "                for entry in metadata:\n",
        "                    wav_name = entry.get(\"Filename\")\n",
        "                    transcript = entry.get(\"Prompt\", {}).get(\"Transcript\", \"\")\n",
        "                    if not wav_name or not transcript:\n",
        "                        continue\n",
        "                    wav_path = os.path.join(root, wav_name)\n",
        "                    reference_dict[wav_path] = transcript.strip().lower()\n",
        "\n",
        "    print(f\"[{root_folder}] Found {len(reference_dict)} reference entries in JSON files\")\n",
        "\n",
        "    # 2) Collect all wav files\n",
        "    wav_files = sorted(\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(root_folder)\n",
        "        for f in files\n",
        "        if f.lower().endswith(\".wav\")\n",
        "    )\n",
        "    print(f\"[{root_folder}] Found {len(wav_files)} wav files\")\n",
        "\n",
        "    # 3) put model on device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    metric = evaluate.load(\"wer\")\n",
        "\n",
        "    def load_audio(path, target_sr=16000):\n",
        "        speech_array, sr = sf.read(path)\n",
        "        if sr != target_sr:\n",
        "            speech_array = librosa.resample(speech_array, orig_sr=sr, target_sr=target_sr)\n",
        "        return speech_array\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def batch_transcribe(wav_paths):\n",
        "        transcriptions = []\n",
        "        for i in tqdm(\n",
        "            range(0, len(wav_paths), batch_size),\n",
        "            desc=f\"Transcribing ({os.path.basename(root_folder)})\"\n",
        "        ):\n",
        "            batch = [load_audio(p) for p in wav_paths[i:i+batch_size]]\n",
        "\n",
        "            # Encode audio -> input_features + attention_mask\n",
        "            encoded = processor(\n",
        "                batch,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "            )\n",
        "\n",
        "            input_features = encoded.input_features.to(\n",
        "                device=device,\n",
        "                dtype=model.dtype,   # <-- match model (float16 on GPU, float32 on CPU)\n",
        "            )\n",
        "\n",
        "            attention_mask = getattr(encoded, \"attention_mask\", None)\n",
        "            if attention_mask is not None:\n",
        "                attention_mask = attention_mask.to(device)\n",
        "\n",
        "                pred_ids = model.generate(\n",
        "                    input_features=input_features,\n",
        "                    attention_mask=attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                pred_ids = model.generate(\n",
        "                    input_features=input_features,\n",
        "                )\n",
        "\n",
        "            decoded = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "            transcriptions.extend(t.strip().lower() for t in decoded)\n",
        "\n",
        "        return transcriptions\n",
        "\n",
        "\n",
        "    preds = batch_transcribe(wav_files)\n",
        "\n",
        "    # 4) compute WER per file\n",
        "    results = []\n",
        "    for wav_path, pred in zip(wav_files, preds):\n",
        "        ref = reference_dict.get(wav_path, \"\")\n",
        "        wer = metric.compute(predictions=[pred], references=[ref])\n",
        "        results.append({\n",
        "            \"Path\": wav_path,\n",
        "            \"Filename\": os.path.basename(wav_path),\n",
        "            \"Prediction\": pred,\n",
        "            \"Reference\": ref,\n",
        "            \"WER\": wer,\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    overall_wer = results_df[\"WER\"].mean()\n",
        "    print(f\"\\n[{os.path.basename(root_folder)}] Overall average WER: {overall_wer:.3f}\")\n",
        "\n",
        "    top10 = results_df.sort_values(\"WER\", ascending=False).head(10)\n",
        "    print(\"\\nTop 10 highest WER samples:\")\n",
        "    print(top10[[\"Filename\", \"WER\", \"Reference\", \"Prediction\"]].to_string(index=False))\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4) Run evaluations\n",
        "# ----------------------------------------------------\n",
        "\n",
        "results_regular_df = transcribe_whisper_and_compute_wer(\n",
        "    DATA_DIR_REGULAR,\n",
        "    model=model_regular,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "results_trimmed_df = transcribe_whisper_and_compute_wer(\n",
        "    DATA_DIR_TRIMMED,\n",
        "    model=model_trimmed,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "baseline_results_regular_df = transcribe_whisper_and_compute_wer(\n",
        "    DATA_DIR_REGULAR,\n",
        "    model=baseline_model,\n",
        "    device=device,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0407a65d54c14f6aa76ebfa46d4f052d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99b34de8d55e4a668d0e7cda6c660a5e",
            "placeholder": "​",
            "style": "IPY_MODEL_6718a72ebfaf4acab58b2db1a0d3b039",
            "value": "Map: 100%"
          }
        },
        "22a89e60bd58475096d94b7e1d7391f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cd8645da7c646f385cfe539574e3784": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c204926838b4863b5d92ebc49a98eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f62841ba2feb4f4d8f2b4100a085b197",
              "IPY_MODEL_cc18d01b14764745ac9acfaac9945e79",
              "IPY_MODEL_55fb8af903d343578044b6f362502498"
            ],
            "layout": "IPY_MODEL_7f1a50638759468dbda943940e207e13"
          }
        },
        "4405237b613d4221a0b47eef7fc2fd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f39fabfc6034c558cbcf0f7c6f23540",
            "max": 207,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cd8645da7c646f385cfe539574e3784",
            "value": 207
          }
        },
        "4f39fabfc6034c558cbcf0f7c6f23540": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55fb8af903d343578044b6f362502498": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e74f993a9a4549eeb41eba3859d6ac96",
            "placeholder": "​",
            "style": "IPY_MODEL_7e446f565ecc4871bbdbba1048de6fe2",
            "value": " 207/207 [00:20&lt;00:00, 13.91 examples/s]"
          }
        },
        "6718a72ebfaf4acab58b2db1a0d3b039": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ef9474a0c984c3c8db93e2d6d17a634": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cfa0779a3d84867956464425d5572b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e446f565ecc4871bbdbba1048de6fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f1a50638759468dbda943940e207e13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80af569074d74c7380385b83e36cc965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8756b02425aa493494ad57b9ec17e3d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b34de8d55e4a668d0e7cda6c660a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b492604d80d14d44ab58b2098364da15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcb4568c50d64caebe9299eca93d6a9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc18d01b14764745ac9acfaac9945e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ef9474a0c984c3c8db93e2d6d17a634",
            "max": 207,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cfa0779a3d84867956464425d5572b5",
            "value": 207
          }
        },
        "cf0b835a58b44d7fb9f3742f628a7af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8756b02425aa493494ad57b9ec17e3d4",
            "placeholder": "​",
            "style": "IPY_MODEL_b492604d80d14d44ab58b2098364da15",
            "value": " 207/207 [00:17&lt;00:00, 17.01 examples/s]"
          }
        },
        "e74f993a9a4549eeb41eba3859d6ac96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b154ac781a4c8ca24847c668f51bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0407a65d54c14f6aa76ebfa46d4f052d",
              "IPY_MODEL_4405237b613d4221a0b47eef7fc2fd78",
              "IPY_MODEL_cf0b835a58b44d7fb9f3742f628a7af2"
            ],
            "layout": "IPY_MODEL_22a89e60bd58475096d94b7e1d7391f3"
          }
        },
        "f62841ba2feb4f4d8f2b4100a085b197": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcb4568c50d64caebe9299eca93d6a9d",
            "placeholder": "​",
            "style": "IPY_MODEL_80af569074d74c7380385b83e36cc965",
            "value": "Map: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
