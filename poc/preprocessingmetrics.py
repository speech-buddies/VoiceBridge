# -*- coding: utf-8 -*-
"""PreprocessingMetrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tWFRDGu0KUBkNdRK1iF-Pd_OPjSmP7oc
"""

#pip install torch torchaudio transformers datasets peft bitsandbytes accelerate torchcodec dataset evaluate bitsandbytes jiwer -q

HF_TOKEN="xxx"  # --- Add Hugging Face Token ---

from transformers import WhisperForConditionalGeneration, WhisperProcessor

model_name = "openai/whisper-small.en"
processor = WhisperProcessor.from_pretrained(model_name)  # (same as original, reused everywhere)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training  # (original import)

# CHANGED: instead of creating a single global model once,
# we wrap the original QLoRA setup into a helper so we can
# create a fresh model *per dataset* (regular vs trimmed).
def create_lora_model():
    """Create a fresh 4-bit QLoRA Whisper model (same config as original)."""
    model = WhisperForConditionalGeneration.from_pretrained(
        model_name,
        load_in_4bit=True,               # quantize base model
        device_map="auto",
    )

    # For QLoRA (same as original)
    model = prepare_model_for_kbit_training(model)  # enables 4-bit quantization

    config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],  # or attention layers depending on model
        lora_dropout=0.1,
        bias="none",
        task_type="SEQ_2_SEQ_LM"  # or "CTC" for wav2vec2
    )

    model = get_peft_model(model, config)
    model.print_trainable_parameters()

    # CHANGED: the forward patch originally lived at global scope;
    # now we apply it inside this helper so each new model is patched.
    from types import MethodType

    def patched_forward(self, input_features=None, **kwargs):
        return self.model.forward(input_features=input_features, **kwargs)

    model.forward = MethodType(patched_forward, model)  # CHANGED: moved from global into helper

    return model  # NEW

# from datasets import load_dataset
from google.colab import drive
import os

drive.mount('/content/drive')

import os, json
from datasets import Dataset, Audio

# ---- CONFIG ----
SAMPLING_RATE = 16_000  # set based on your ASR model

# NEW: two dataset directories instead of one, as per your requirement.
DATA_DIR_REGULAR = '/content/drive/My Drive/VoiceBridge_SAP_sample'          # CHANGED: was VoiceBridge_SAP_sample
DATA_DIR_TRIMMED = '/content/drive/My Drive/VoiceBridge_SAP_sample_trimmed'  # NEW
META_FILENAME = "digital_assistant_metadata.json"  # NEW: common metadata filename


# NEW: factor out the original metadata + dataset-building logic so we can reuse it.
def build_dataset_from_dir(data_dir: str):
    """
    Load metadata and build a HuggingFace Dataset from a given directory.
    This is the same logic as your original code, but parameterized by `data_dir`.
    """
    meta_file = os.path.join(data_dir, META_FILENAME)  # CHANGED: use parameterized dir

    # ---- LOAD METADATA ----
    with open(meta_file, "r") as f:
        metadata = json.load(f)

    # ---- BUILD LIST OF AUDIO–TEXT PAIRS ----
    data = []
    for item in metadata:
        filename = item.get("Filename")
        transcript = item.get("Prompt", {}).get("Transcript", "")
        if not filename or not transcript:
            continue
        audio_path = os.path.join(data_dir, filename)  # CHANGED: was DATA_DIR
        if os.path.exists(audio_path):
            data.append({"audio": audio_path, "sentence": transcript})

    print(f"[{data_dir}] Found {len(data)} usable audio–transcript pairs")

    # ---- CREATE HUGGING FACE DATASET ----
    dataset = Dataset.from_list(data)
    # dataset = dataset.cast_column("audio", Audio(sampling_rate=SAMPLING_RATE))
    dataset = dataset.cast_column("audio", Audio(sampling_rate=SAMPLING_RATE, decode=False))
    return dataset  # NEW

# (kept from original)
from transformers import WhisperProcessor
processor = WhisperProcessor.from_pretrained("openai/whisper-small.en")

import torchaudio


def preprocess(batch):
    speech_arrays = []
    for audio_info in batch["audio"]:
        path = audio_info["path"]
        speech_array, sr = torchaudio.load(path)

        # CHANGED: always downmix to mono so every sample is 1D
        if speech_array.ndim == 2:               # (channels, samples)
            # average across channels -> mono
            speech_array = speech_array.mean(dim=0)

        # CHANGED: ensure we end up with a 1D numpy array
        speech_arrays.append(speech_array.numpy())

    # Feature extraction: do NOT use return_tensors here
    input_features = processor.feature_extractor(
        speech_arrays,
        sampling_rate=SAMPLING_RATE,
    ).input_features  # list of arrays

    # Tokenize transcripts (unchanged)
    labels = processor.tokenizer(
        batch["sentence"],
        add_special_tokens=True
    ).input_ids

    return {"input_features": input_features, "labels": labels}

from dataclasses import dataclass
from typing import Any, Dict, List, Union
import torch


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        labels_batch = [{"input_ids": f["labels"]} for f in features]

        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels = self.processor.tokenizer.pad(labels_batch, return_tensors="pt")["input_ids"]

        batch["labels"] = labels.masked_fill(labels == self.processor.tokenizer.pad_token_id, -100)
        return batch


data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

import numpy as np
import evaluate

wer_metric = evaluate.load("wer")


def compute_metrics(pred):
    pred_ids = np.argmax(pred.predictions, axis=-1)
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)
    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

from transformers import TrainingArguments, Trainer


# NEW: wrapper to do the *entire* training pipeline for a given folder.
def train_q_lora_for_folder(data_dir: str, output_dir_root: str):
    """
    Train a QLoRA Whisper model on the dataset in `data_dir`.
    This wraps your original "dataset + trainer + train + save" code so
    we can call it for both regular and trimmed folders.
    """
    print(f"\n=== Training QLoRA model for: {data_dir} ===")

    # 1) Build dataset (original logic, now via helper)
    dataset = build_dataset_from_dir(data_dir)  # NEW

    # 2) Map preprocess (same as original)
    dataset = dataset.map(
        preprocess,
        # remove_columns=["audio", "sentence"],
        batched=True,
        batch_size=4,
        num_proc=1,
    )

    # 3) Split train / eval (same as original)
    dataset = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = dataset["train"]
    eval_dataset = dataset["test"]

    # 4) Create a fresh LoRA model (NEW vs original single global model)
    model = create_lora_model()  # NEW

    # 5) Optional: quick forward pass check (same logic as original)
    batch = data_collator([train_dataset[0], train_dataset[1]])
    batch = {k: v.to(model.device) for k, v in batch.items()}

    outputs = model(
        input_features=batch["input_features"],
        labels=batch["labels"]
    )
    print("Sanity check loss:", outputs.loss.item())

    # 6) Training arguments (same as original, but output_dir is parameterized)
    training_args = TrainingArguments(
        output_dir=output_dir_root,          # CHANGED: was "./whisper-lora-checkpoints"
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        learning_rate=1e-4,
        num_train_epochs=3,
        fp16=False,
        bf16=False,
        logging_strategy="steps",
        logging_steps=10,
        eval_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        report_to="none",
        load_best_model_at_end=True,
    )

    # 7) Trainer (same as original)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=None,
        compute_metrics=compute_metrics,
    )

    # 8) Train (same as original)
    trainer.train()

    # 9) Save adapters and merged model (same logic, but under output_dir_root)
    adapter_dir = os.path.join(output_dir_root, "whisper-lora-adapter")   # NEW
    merged_dir = os.path.join(output_dir_root, "whisper-lora-merged")     # NEW

    model.save_pretrained(adapter_dir)

    merged_model = model.merge_and_unload()
    merged_model.save_pretrained(merged_dir)

    processor.save_pretrained(merged_dir)

    print(f"Saved adapter to: {adapter_dir}")
    print(f"Saved merged model to: {merged_dir}")

    # Return merged_model so we can evaluate WER on it
    return merged_model, merged_dir, eval_dataset  # NEW

OUTPUT_DIR_REGULAR = "/content/drive/My Drive/whisper-lora-regular-checkpoints"

model_regular, merged_dir_regular, eval_dataset_regular = train_q_lora_for_folder(
    DATA_DIR_REGULAR,
    OUTPUT_DIR_REGULAR,
)

OUTPUT_DIR_TRIMMED = "/content/drive/My Drive/whisper-lora-trimmed-checkpoints"

model_trimmed, merged_dir_trimmed, eval_dataset_trimmed = train_q_lora_for_folder(
    DATA_DIR_TRIMMED,
    OUTPUT_DIR_TRIMMED,
)

# =========================
# Merge LoRA checkpoints and test the models
# =========================

import os
import json
import soundfile as sf
import librosa
import pandas as pd
from tqdm import tqdm
import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from peft import PeftModel
import evaluate

# --- paths (must match what you used in training) ---
DATA_DIR_REGULAR = "/content/drive/My Drive/VoiceBridge_SAP_sample"
DATA_DIR_TRIMMED = "/content/drive/My Drive/VoiceBridge_SAP_sample_trimmed"

# These are the *parent* dirs where training outputs live
OUTPUT_DIR_REGULAR = "/content/drive/My Drive/whisper-lora-regular-checkpoints"
OUTPUT_DIR_TRIMMED = "/content/drive/My Drive/whisper-lora-trimmed-checkpoints"

# These are the LoRA adapter dirs (where adapter_config.json actually is)
ADAPTER_DIR_REGULAR = os.path.join(OUTPUT_DIR_REGULAR, "whisper-lora-adapter")
ADAPTER_DIR_TRIMMED = os.path.join(OUTPUT_DIR_TRIMMED, "whisper-lora-adapter")

# Where we'll save merged full models
MERGED_DIR_REGULAR = os.path.join(OUTPUT_DIR_REGULAR, "whisper-lora-merged")
MERGED_DIR_TRIMMED = os.path.join(OUTPUT_DIR_TRIMMED, "whisper-lora-merged")

BASE_MODEL_ID = "openai/whisper-small.en"

# ----------------------------------------------------
# 1) Merge LoRA adapters into full Whisper models
# ----------------------------------------------------

processor_base = WhisperProcessor.from_pretrained(BASE_MODEL_ID)

def merge_lora_checkpoint(adapter_dir: str, merged_dir: str):
    """
    Load base Whisper, attach LoRA from `adapter_dir`, merge, and save
    a normal (non-quantized) model into `merged_dir`.
    """
    print(f"\n[MERGE] From adapter dir: {adapter_dir}")
    print(f"[MERGE] To merged dir:    {merged_dir}")

    if not os.path.isdir(adapter_dir):
        raise FileNotFoundError(f"Adapter dir not found: {adapter_dir}")

    # Load base model in full precision (no bitsandbytes)
    base_model = WhisperForConditionalGeneration.from_pretrained(
        BASE_MODEL_ID,
        torch_dtype=torch.float32,
    )

    # Attach LoRA weights from adapter directory
    lora_model = PeftModel.from_pretrained(base_model, adapter_dir)

    # Merge LoRA into the base weights
    lora_model = lora_model.merge_and_unload()

    # Save merged model + processor
    os.makedirs(merged_dir, exist_ok=True)
    lora_model.to("cpu")
    lora_model.save_pretrained(merged_dir)
    processor_base.save_pretrained(merged_dir)

    print("[MERGE] Done.")

# Merge both REGULAR and TRIMMED LoRA checkpoints
merge_lora_checkpoint(ADAPTER_DIR_REGULAR, MERGED_DIR_REGULAR)
merge_lora_checkpoint(ADAPTER_DIR_TRIMMED, MERGED_DIR_TRIMMED)

print("\n[MERGE] Finished merging both models.\n")

# ----------------------------------------------------
# 2) Load merged models (no quantization, no bitsandbytes)
# ----------------------------------------------------

processor = WhisperProcessor.from_pretrained(BASE_MODEL_ID)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

model_regular = WhisperForConditionalGeneration.from_pretrained(
    MERGED_DIR_REGULAR,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
)
model_regular.to(device)

model_trimmed = WhisperForConditionalGeneration.from_pretrained(
    MERGED_DIR_TRIMMED,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
)
model_trimmed.to(device)

baseline_model = WhisperForConditionalGeneration.from_pretrained(
    BASE_MODEL_ID,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
)
baseline_model.to(device)

# ----------------------------------------------------
# 3) WER function for Whisper (nested folder layout)
# ----------------------------------------------------

def transcribe_whisper_and_compute_wer(
    root_folder,
    model,
    device=None,
    batch_size=4,
):
    """
    Walk `root_folder`, find all .json files, build:
        abs wav path -> transcript
    Then transcribe all wavs and compute WER per file.
    """
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")

    # 1) Build reference dict
    reference_dict = {}
    for root, _, files in os.walk(root_folder):
        for fname in files:
            if fname.lower().endswith(".json"):
                json_path = os.path.join(root, fname)
                with open(json_path, "r") as f:
                    metadata = json.load(f)

                for entry in metadata:
                    wav_name = entry.get("Filename")
                    transcript = entry.get("Prompt", {}).get("Transcript", "")
                    if not wav_name or not transcript:
                        continue
                    wav_path = os.path.join(root, wav_name)
                    reference_dict[wav_path] = transcript.strip().lower()

    print(f"[{root_folder}] Found {len(reference_dict)} reference entries in JSON files")

    # 2) Collect all wav files
    wav_files = sorted(
        os.path.join(root, f)
        for root, _, files in os.walk(root_folder)
        for f in files
        if f.lower().endswith(".wav")
    )
    print(f"[{root_folder}] Found {len(wav_files)} wav files")

    # 3) put model on device
    model.to(device)
    model.eval()

    metric = evaluate.load("wer")

    def load_audio(path, target_sr=16000):
        speech_array, sr = sf.read(path)
        if sr != target_sr:
            speech_array = librosa.resample(speech_array, orig_sr=sr, target_sr=target_sr)
        return speech_array

    @torch.inference_mode()
    def batch_transcribe(wav_paths):
        transcriptions = []
        for i in tqdm(
            range(0, len(wav_paths), batch_size),
            desc=f"Transcribing ({os.path.basename(root_folder)})"
        ):
            batch = [load_audio(p) for p in wav_paths[i:i+batch_size]]

            # Encode audio -> input_features + attention_mask
            encoded = processor(
                batch,
                sampling_rate=16000,
                return_tensors="pt",
                padding=True,
            )

            input_features = encoded.input_features.to(
                device=device,
                dtype=model.dtype,   # <-- match model (float16 on GPU, float32 on CPU)
            )

            attention_mask = getattr(encoded, "attention_mask", None)
            if attention_mask is not None:
                attention_mask = attention_mask.to(device)

                pred_ids = model.generate(
                    input_features=input_features,
                    attention_mask=attention_mask,
                )
            else:
                pred_ids = model.generate(
                    input_features=input_features,
                )

            decoded = processor.batch_decode(pred_ids, skip_special_tokens=True)
            transcriptions.extend(t.strip().lower() for t in decoded)

        return transcriptions


    preds = batch_transcribe(wav_files)

    # 4) compute WER per file
    results = []
    for wav_path, pred in zip(wav_files, preds):
        ref = reference_dict.get(wav_path, "")
        wer = metric.compute(predictions=[pred], references=[ref])
        results.append({
            "Path": wav_path,
            "Filename": os.path.basename(wav_path),
            "Prediction": pred,
            "Reference": ref,
            "WER": wer,
        })

    results_df = pd.DataFrame(results)

    overall_wer = results_df["WER"].mean()
    print(f"\n[{os.path.basename(root_folder)}] Overall average WER: {overall_wer:.3f}")

    top10 = results_df.sort_values("WER", ascending=False).head(10)
    print("\nTop 10 highest WER samples:")
    print(top10[["Filename", "WER", "Reference", "Prediction"]].to_string(index=False))

    return results_df

# ----------------------------------------------------
# 4) Run evaluations
# ----------------------------------------------------

results_regular_df = transcribe_whisper_and_compute_wer(
    DATA_DIR_REGULAR,
    model=model_regular,
    device=device,
)

results_trimmed_df = transcribe_whisper_and_compute_wer(
    DATA_DIR_TRIMMED,
    model=model_trimmed,
    device=device,
)

baseline_results_regular_df = transcribe_whisper_and_compute_wer(
    DATA_DIR_REGULAR,
    model=baseline_model,
    device=device,
)