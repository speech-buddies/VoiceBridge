{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ef1ee-3c9b-4b44-a4ba-e737c3c56cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchaudio transformers peft torchcodec evaluate jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903d80c-b500-4a9d-972a-34402bb55fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TORCHCODEC_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c13d11-69cb-4ac2-91cc-4dc075c1881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "\n",
    "# ---- CONFIG ----\n",
    "DATA_ARCHIVE = \"./digital_assistant_prompt_samples.tar\"  # path to your .tar\n",
    "DATA_DIR = \"./digital_assistant_prompt_samples\"           # extraction folder\n",
    "META_FILE_NAME = \"digital_assistant_metadata.json\"\n",
    "SAMPLING_RATE = 16_000  # typical for Whisper-small\n",
    "\n",
    "# ---- EXTRACT TAR IF NOT ALREADY DONE ----\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(f\"Extracting {DATA_ARCHIVE} to {DATA_DIR}...\")\n",
    "    with tarfile.open(DATA_ARCHIVE, \"r\") as tar:\n",
    "        tar.extractall(path=os.path.dirname(DATA_DIR))\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(f\"{DATA_DIR} already exists, skipping extraction.\")\n",
    "\n",
    "    \n",
    "META_FILE = os.path.join(DATA_DIR, \"digital_assistant_metadata.json\")\n",
    "SAMPLING_RATE = 16_000\n",
    "\n",
    "# Load metadata\n",
    "with open(META_FILE, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Build list of (audio_path, transcript) pairs\n",
    "data = []\n",
    "for item in metadata:\n",
    "    filename = item.get(\"Filename\")\n",
    "    transcript = item.get(\"Prompt\", {}).get(\"Transcript\", \"\")\n",
    "    if not filename or not transcript:\n",
    "        continue\n",
    "    audio_path = os.path.join(DATA_DIR, filename)\n",
    "    if os.path.exists(audio_path):\n",
    "        data.append((audio_path, transcript))\n",
    "\n",
    "print(f\"Found {len(data)} usable audio files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9370b-286f-43ac-af6c-a834d9270910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "TEST_SPLIT = 0.1\n",
    "SEED = 42\n",
    "\n",
    "# ---- 3. Shuffle and split into train/test ----\n",
    "import random\n",
    "\n",
    "random.seed(SEED)\n",
    "shuffled_data = data.copy()\n",
    "random.shuffle(shuffled_data)\n",
    "\n",
    "split_idx = int(len(shuffled_data) * (1 - TEST_SPLIT))\n",
    "train_data = shuffled_data[:split_idx]\n",
    "test_data = shuffled_data[split_idx:]\n",
    "\n",
    "# ---- 4. Define PyTorch Dataset ----\n",
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, data_list, processor, sampling_rate=SAMPLING_RATE):\n",
    "        self.data_list = data_list\n",
    "        self.processor = processor\n",
    "        self.sr = sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, transcript = self.data_list[idx]\n",
    "\n",
    "        # Load audio and resample if necessary\n",
    "        waveform, sr = sf.read(audio_path)\n",
    "        waveform = torch.tensor(waveform).float()\n",
    "        \n",
    "        # If stereo → average or take first channel\n",
    "        if waveform.ndim > 1:\n",
    "            waveform = waveform.mean(dim=1)\n",
    "        if sr != self.sr:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sr)\n",
    "        waveform = waveform.squeeze(0).numpy()  # remove channel dim\n",
    "\n",
    "        # Feature extraction\n",
    "        input_features = self.processor.feature_extractor(\n",
    "            waveform,\n",
    "            sampling_rate=self.sr\n",
    "        ).input_features[0]  # single array\n",
    "\n",
    "        # Tokenize transcript\n",
    "        labels = self.processor.tokenizer(\n",
    "            transcript,\n",
    "            add_special_tokens=True\n",
    "        ).input_ids\n",
    "\n",
    "        return {\"input_features\": torch.tensor(input_features, dtype=torch.float32),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.long)}\n",
    "\n",
    "# ---- 5. Initialize processor ----\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# ---- 6. Create train/test datasets ----\n",
    "train_dataset = AudioTextDataset(train_data, processor)\n",
    "test_dataset = AudioTextDataset(test_data, processor)\n",
    "\n",
    "\n",
    "# ---- 7. Create DataLoaders ----\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: {\n",
    "    \"input_features\": torch.nn.utils.rnn.pad_sequence([item[\"input_features\"] for item in x], batch_first=True),\n",
    "    \"labels\": torch.nn.utils.rnn.pad_sequence([item[\"labels\"] for item in x], batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "})\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: {\n",
    "    \"input_features\": torch.nn.utils.rnn.pad_sequence([item[\"input_features\"] for item in x], batch_first=True),\n",
    "    \"labels\": torch.nn.utils.rnn.pad_sequence([item[\"labels\"] for item in x], batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "})\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1957a-f535-4db4-890a-26e4529ce7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" \n",
    "# ---- 7. Load Whisper-small model ----\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---- 8. Apply LoRA (no quantization) ----\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c939e4-559c-48c9-ab41-54e277dc0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        labels_batch = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.pad(labels_batch, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "        batch[\"labels\"] = labels.masked_fill(labels == self.processor.tokenizer.pad_token_id, -100)\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66833ad0-6feb-43de-8c1f-383dbda6e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "# Patch forward pass to accept input features\n",
    "def patched_forward(self, input_features=None, **kwargs):\n",
    "    return self.model.forward(input_features=input_features, **kwargs)\n",
    "\n",
    "model.forward = MethodType(patched_forward, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904a168-5a91-4467-ba82-f823094b4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([train_dataset[0], train_dataset[1]])\n",
    "batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "outputs = model(\n",
    "    input_features=batch[\"input_features\"],\n",
    "    labels=batch[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580165a-9e05-4b36-85ff-a457cc5259a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ---- 9. Define optimizer ----\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ---- 10. Training loop with progress prints and checkpointing ----\n",
    "EPOCHS = 3  # adjust as needed\n",
    "SAVE_DIR = \"./checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "PRINT_EVERY = 10        # print every N batches\n",
    "CHECKPOINT_EVERY = 100  # save checkpoint every N batches\n",
    "\n",
    "global_step = 0  # track batches across epochs\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(train_loader, start=1):\n",
    "        global_step += 1\n",
    "\n",
    "        input_features = batch[\"input_features\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # ---- PRINT PROGRESS ----\n",
    "        if i % PRINT_EVERY == 0:\n",
    "            avg_loss = running_loss / PRINT_EVERY\n",
    "            print(f\"[Epoch {epoch+1} Batch {i}/{len(train_loader)}] Avg Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "        # ---- SAVE CHECKPOINT EVERY 100 BATCHES ----\n",
    "        if global_step % CHECKPOINT_EVERY == 0:\n",
    "            checkpoint_path = os.path.join(SAVE_DIR, f\"checkpoint_step{global_step}.pt\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved checkpoint at step {global_step} → {checkpoint_path}\")\n",
    "\n",
    "    # ---- SAVE END-OF-EPOCH CHECKPOINT ----\n",
    "    epoch_ckpt = os.path.join(SAVE_DIR, f\"whisper_lora_epoch{epoch+1}.pt\")\n",
    "    torch.save(model.state_dict(), epoch_ckpt)\n",
    "    print(f\"Epoch {epoch+1} finished. Checkpoint saved to {epoch_ckpt}\")\n",
    "\n",
    "# ---- 11. Optional: evaluation with incremental prints ----\n",
    "model.eval()\n",
    "eval_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader, start=1):\n",
    "        input_features = batch[\"input_features\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        if i % PRINT_EVERY == 0:\n",
    "            avg_loss = eval_loss / i\n",
    "            print(f\"[Eval Batch {i}/{len(test_loader)}] Avg Loss so far: {avg_loss:.4f}\")\n",
    "\n",
    "final_eval_loss = eval_loss / len(test_loader)\n",
    "print(f\"Final Evaluation Loss: {final_eval_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04f172-6bca-412a-8d2c-e7fc746701c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
